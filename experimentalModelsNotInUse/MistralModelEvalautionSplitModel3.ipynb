{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7bd8438bd7e441c08202640bf83e1140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1760f7189b3941e48e636d11345033f6",
              "IPY_MODEL_a67de702761e41cd88b7f279850b4f8f",
              "IPY_MODEL_18f9d8baf861487d868d8b49a6ad3d9d"
            ],
            "layout": "IPY_MODEL_7efe0fbcd6a245d5b93f32308d321e67"
          }
        },
        "1760f7189b3941e48e636d11345033f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d71b4aca1aaa4cbe99c90189d0891058",
            "placeholder": "​",
            "style": "IPY_MODEL_85d28fe8de554eba84ec230397d0c03f",
            "value": "Map: 100%"
          }
        },
        "a67de702761e41cd88b7f279850b4f8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_748141bfdd3b42a9b219c3b4b982ea6c",
            "max": 503,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dce0e50f1a9f4a36a6f77de5ebe5370e",
            "value": 503
          }
        },
        "18f9d8baf861487d868d8b49a6ad3d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faed3e7bb2df48c69d1e4c54a17a9d05",
            "placeholder": "​",
            "style": "IPY_MODEL_9f109737b2844f9e97db76679689927f",
            "value": " 503/503 [00:00&lt;00:00, 623.54 examples/s]"
          }
        },
        "7efe0fbcd6a245d5b93f32308d321e67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d71b4aca1aaa4cbe99c90189d0891058": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85d28fe8de554eba84ec230397d0c03f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "748141bfdd3b42a9b219c3b4b982ea6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dce0e50f1a9f4a36a6f77de5ebe5370e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "faed3e7bb2df48c69d1e4c54a17a9d05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f109737b2844f9e97db76679689927f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2291fda5d7c4281ac0a35ae067ab1bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8c129df57cd4b78bd48eec1f69caf6a",
              "IPY_MODEL_dc66e629cc934b969e3bfb7466619315",
              "IPY_MODEL_cd8496092c384a3db8b84340be839fde"
            ],
            "layout": "IPY_MODEL_b54b020120bf4d98b19ebe76abb8b74d"
          }
        },
        "d8c129df57cd4b78bd48eec1f69caf6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c23fdc4c13ea4ccf9f31d498c6b14f18",
            "placeholder": "​",
            "style": "IPY_MODEL_4517819dfe4a424db1fc4e6bb1a534fe",
            "value": "Map: 100%"
          }
        },
        "dc66e629cc934b969e3bfb7466619315": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_258022a102a6408a92f3070cfff11772",
            "max": 56,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d3ded2e50414642917be3a8ecee84ec",
            "value": 56
          }
        },
        "cd8496092c384a3db8b84340be839fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaa9d831e0e947ae832a2f3c65f1d3cc",
            "placeholder": "​",
            "style": "IPY_MODEL_c70f180037444f849b3af9e845ead688",
            "value": " 56/56 [00:00&lt;00:00, 588.60 examples/s]"
          }
        },
        "b54b020120bf4d98b19ebe76abb8b74d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c23fdc4c13ea4ccf9f31d498c6b14f18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4517819dfe4a424db1fc4e6bb1a534fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "258022a102a6408a92f3070cfff11772": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d3ded2e50414642917be3a8ecee84ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aaa9d831e0e947ae832a2f3c65f1d3cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c70f180037444f849b3af9e845ead688": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "058267e978dc4fbfb2f4115f5b771102": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b32596174f24232bdb429e1dcc71986",
              "IPY_MODEL_7d27b9fa37234934a6739669fc27d53b",
              "IPY_MODEL_d6fcf9c22fe642b28ec121ee49008c4c"
            ],
            "layout": "IPY_MODEL_a5d23f1d45234e30aa62d43dca10e683"
          }
        },
        "4b32596174f24232bdb429e1dcc71986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_536d64bbae2d46f08b5ec012fd5831b2",
            "placeholder": "​",
            "style": "IPY_MODEL_023f7b4e14fb4adda6a81bd90cb04c83",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7d27b9fa37234934a6739669fc27d53b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_114b5bdba51f4b098b9e3367da6e410f",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a52e722d23684553a22a4cff327b1568",
            "value": 3
          }
        },
        "d6fcf9c22fe642b28ec121ee49008c4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_980933b32a8c43e3aaeefd2eab6108a6",
            "placeholder": "​",
            "style": "IPY_MODEL_f2f8718ce8b8418f89cf87148a36a254",
            "value": " 3/3 [00:17&lt;00:00,  5.65s/it]"
          }
        },
        "a5d23f1d45234e30aa62d43dca10e683": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "536d64bbae2d46f08b5ec012fd5831b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "023f7b4e14fb4adda6a81bd90cb04c83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "114b5bdba51f4b098b9e3367da6e410f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a52e722d23684553a22a4cff327b1568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "980933b32a8c43e3aaeefd2eab6108a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2f8718ce8b8418f89cf87148a36a254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30fcdfe95d42446fb319571e91a80708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_a03a051c4b454e6480fd3eefacdf64c7"
          }
        },
        "098ee563c896466b8667ee6161379770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d50aaf5cbf594dd2aaf360fa21001381",
            "placeholder": "​",
            "style": "IPY_MODEL_72a6c4d06bb4430ea5be9f9ba60ada75",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "b2adbc8bd50744f58eb60e9a6699554b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_62a9799066d34c43b0b15556b6ba56ee",
            "placeholder": "​",
            "style": "IPY_MODEL_eef8f4cdb97043869ae6c8e9ebca206c",
            "value": ""
          }
        },
        "a5b57670b91e4a9e9baf162292678e5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_86676ca5b48a44f8bf4b13773ffb1fb3",
            "style": "IPY_MODEL_3597e04d0f0346deb50ec51832bba5d8",
            "value": true
          }
        },
        "4005046b521846459e567803b27b4e12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_67c5457606014e3faca3a17ca06c1e4d",
            "style": "IPY_MODEL_40d56a88db974d689f51d8976267b4cf",
            "tooltip": ""
          }
        },
        "415b1778e87e4a5891958b3a9ae82ab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecbf4ffd4d2f4c418e9f0e4de74f92c6",
            "placeholder": "​",
            "style": "IPY_MODEL_711184e138d3405c9efcdd0ab4028e93",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "a03a051c4b454e6480fd3eefacdf64c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "d50aaf5cbf594dd2aaf360fa21001381": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72a6c4d06bb4430ea5be9f9ba60ada75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62a9799066d34c43b0b15556b6ba56ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eef8f4cdb97043869ae6c8e9ebca206c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86676ca5b48a44f8bf4b13773ffb1fb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3597e04d0f0346deb50ec51832bba5d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67c5457606014e3faca3a17ca06c1e4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40d56a88db974d689f51d8976267b4cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "ecbf4ffd4d2f4c418e9f0e4de74f92c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "711184e138d3405c9efcdd0ab4028e93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ab7e0bd1432451787fe949d0c83efca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d762a0ad00d4039ac74bcb2933ca337",
            "placeholder": "​",
            "style": "IPY_MODEL_89717355cdef4a9d86e03274b8138366",
            "value": "Connecting..."
          }
        },
        "6d762a0ad00d4039ac74bcb2933ca337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89717355cdef4a9d86e03274b8138366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "590318846eb4486a966bafaef493a366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1fe82fb3a739474d94f9bd6d58585735",
              "IPY_MODEL_058562aa15bd470a8ebede26e47212f8",
              "IPY_MODEL_a577a70be12e4e6a866fcac2c77ca2ee"
            ],
            "layout": "IPY_MODEL_2e2cefff4bbd419c87421fe891602198"
          }
        },
        "1fe82fb3a739474d94f9bd6d58585735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3862f505944423dbf213b4168925b79",
            "placeholder": "​",
            "style": "IPY_MODEL_0248d86b8cf14b64a080829f8e29d183",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "058562aa15bd470a8ebede26e47212f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f597c15e4dc34b729606d7d404e2fa49",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f76e0a8750d04a71ba9c613fce3d4018",
            "value": 3
          }
        },
        "a577a70be12e4e6a866fcac2c77ca2ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b080442691a84a6db07e9ea1f4aae8b5",
            "placeholder": "​",
            "style": "IPY_MODEL_3adf3bcc0e3a4db59db221c605227767",
            "value": " 3/3 [00:16&lt;00:00,  5.60s/it]"
          }
        },
        "2e2cefff4bbd419c87421fe891602198": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3862f505944423dbf213b4168925b79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0248d86b8cf14b64a080829f8e29d183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f597c15e4dc34b729606d7d404e2fa49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f76e0a8750d04a71ba9c613fce3d4018": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b080442691a84a6db07e9ea1f4aae8b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3adf3bcc0e3a4db59db221c605227767": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32c46e3828434632b8deacc01f436648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a44d5374ebe49018e204215b35d46fb",
              "IPY_MODEL_6159c078713742c0bc6d8ae68e017552",
              "IPY_MODEL_a343a41c20034de68ca2808b9400a016"
            ],
            "layout": "IPY_MODEL_12701f041ec741958df18bd25f79e30a"
          }
        },
        "7a44d5374ebe49018e204215b35d46fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fadd2e1656c44c291645761bdffc6d2",
            "placeholder": "​",
            "style": "IPY_MODEL_0df50454c9604bc5b509e10de5638f3a",
            "value": "config.json: 100%"
          }
        },
        "6159c078713742c0bc6d8ae68e017552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cafc1470ecad41269f81325ffe776e5d",
            "max": 596,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b9367adf5b2464b8b2c7ff157263ce4",
            "value": 596
          }
        },
        "a343a41c20034de68ca2808b9400a016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74346ee891a541da967147965a985aba",
            "placeholder": "​",
            "style": "IPY_MODEL_1d46ebbbcd9b443585640b73b6bcc6eb",
            "value": " 596/596 [00:00&lt;00:00, 71.6kB/s]"
          }
        },
        "12701f041ec741958df18bd25f79e30a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fadd2e1656c44c291645761bdffc6d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0df50454c9604bc5b509e10de5638f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cafc1470ecad41269f81325ffe776e5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b9367adf5b2464b8b2c7ff157263ce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74346ee891a541da967147965a985aba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d46ebbbcd9b443585640b73b6bcc6eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f75bf81e9b846519d737c2f537e6e3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3805760f38e646ffac61971c0e89a958",
              "IPY_MODEL_c20a6d0f1103421f9fff0d237efb1aa4",
              "IPY_MODEL_82fa57f141e84d52a0be8bdbf6b40a22"
            ],
            "layout": "IPY_MODEL_848afb66280e42c4a618c9da31e03618"
          }
        },
        "3805760f38e646ffac61971c0e89a958": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c48ab7ff00e4a92a0e7b8aaacd093b8",
            "placeholder": "​",
            "style": "IPY_MODEL_aef459335b424085928ea608b7685c07",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "c20a6d0f1103421f9fff0d237efb1aa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65a3128a9c584e89a73b437ded813936",
            "max": 25125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8cbdf6f9e09a46d4a37c1d7ff5aa5bcb",
            "value": 25125
          }
        },
        "82fa57f141e84d52a0be8bdbf6b40a22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25ab9376cbd5489b87b8f15a149a03ef",
            "placeholder": "​",
            "style": "IPY_MODEL_ebfc13aa097b41b9b80f2c2d83bc30bd",
            "value": " 25.1k/25.1k [00:00&lt;00:00, 3.07MB/s]"
          }
        },
        "848afb66280e42c4a618c9da31e03618": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c48ab7ff00e4a92a0e7b8aaacd093b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aef459335b424085928ea608b7685c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65a3128a9c584e89a73b437ded813936": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cbdf6f9e09a46d4a37c1d7ff5aa5bcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25ab9376cbd5489b87b8f15a149a03ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebfc13aa097b41b9b80f2c2d83bc30bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa17f75330214ef7b7aef03f8b6160e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6257c0adfe1942adb54c85027e6af6fe",
              "IPY_MODEL_17122505fa7249b09e2ea2de19b7fd19",
              "IPY_MODEL_3154fcb147cf4a79b86e4a6ab06a9270"
            ],
            "layout": "IPY_MODEL_fe735a29e4db45aeaba48427e8c6f12e"
          }
        },
        "6257c0adfe1942adb54c85027e6af6fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a011c0f257f64b6688a8f26dd838b6c8",
            "placeholder": "​",
            "style": "IPY_MODEL_0e2e4ca84dc744e285e86c7851232c86",
            "value": "Fetching 3 files: 100%"
          }
        },
        "17122505fa7249b09e2ea2de19b7fd19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aeed61c8cd26440b881f86ab93ba420b",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe88eee57c9646d4b345657809b2f1ca",
            "value": 3
          }
        },
        "3154fcb147cf4a79b86e4a6ab06a9270": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59b1e44429e44cbf969ddba0a498201a",
            "placeholder": "​",
            "style": "IPY_MODEL_00e441735d9f4ad19274d4f94593d549",
            "value": " 3/3 [00:37&lt;00:00, 37.75s/it]"
          }
        },
        "fe735a29e4db45aeaba48427e8c6f12e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a011c0f257f64b6688a8f26dd838b6c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e2e4ca84dc744e285e86c7851232c86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aeed61c8cd26440b881f86ab93ba420b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe88eee57c9646d4b345657809b2f1ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "59b1e44429e44cbf969ddba0a498201a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00e441735d9f4ad19274d4f94593d549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e5e8d860b7447b6a35f7e691b9f644a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac70a5d9d6f34a7bbb4767a56ac407ba",
              "IPY_MODEL_c68a4e4e91e34c0fab031035fef14063",
              "IPY_MODEL_49fd2197a3224be2aae5da4c0ab883ea"
            ],
            "layout": "IPY_MODEL_1b76514142a145bda5e9a59a296b5f04"
          }
        },
        "ac70a5d9d6f34a7bbb4767a56ac407ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3dce121f13649c4ae99fe8b4f84a45c",
            "placeholder": "​",
            "style": "IPY_MODEL_b2086b51bbb64df5806662215c24cd81",
            "value": "model-00003-of-00003.safetensors: 100%"
          }
        },
        "c68a4e4e91e34c0fab031035fef14063": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60a0001935db43828b7713e2382de6af",
            "max": 4540516344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08516d766a1949f49b41809c5a587c28",
            "value": 4540516344
          }
        },
        "49fd2197a3224be2aae5da4c0ab883ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b5f745d341f4708b2b13ae90f1d886b",
            "placeholder": "​",
            "style": "IPY_MODEL_533ed34147854cdca8316e7c3755aa48",
            "value": " 4.54G/4.54G [00:33&lt;00:00, 123MB/s]"
          }
        },
        "1b76514142a145bda5e9a59a296b5f04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3dce121f13649c4ae99fe8b4f84a45c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2086b51bbb64df5806662215c24cd81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60a0001935db43828b7713e2382de6af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08516d766a1949f49b41809c5a587c28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b5f745d341f4708b2b13ae90f1d886b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "533ed34147854cdca8316e7c3755aa48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f234d443dda4c7cb415cc9f5f8bc295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2fcd5c0743d9423b9b62ddd8d6ed9709",
              "IPY_MODEL_3fac4a25b11b4a959e66f643ed110a0b",
              "IPY_MODEL_f87ce0248c0f42cb81c9eb9cfdadf794"
            ],
            "layout": "IPY_MODEL_ea3d0ebefd2d47b8bf9baede6f70c945"
          }
        },
        "2fcd5c0743d9423b9b62ddd8d6ed9709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_774c3e2d9dac4d2b8f73cb0298aa34c4",
            "placeholder": "​",
            "style": "IPY_MODEL_cb8d88ef1ea1469a85a2b4871cee7825",
            "value": "model-00001-of-00003.safetensors: 100%"
          }
        },
        "3fac4a25b11b4a959e66f643ed110a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c380c2b1e1e54f93a6778b5ed4f65912",
            "max": 4943162336,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af5867865649400ea97c3c809b988d90",
            "value": 4943162336
          }
        },
        "f87ce0248c0f42cb81c9eb9cfdadf794": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bffb06e55ccb4126abeefad137ebf296",
            "placeholder": "​",
            "style": "IPY_MODEL_247d83557bf54318ae060f5cd0465ab3",
            "value": " 4.94G/4.94G [00:37&lt;00:00, 206MB/s]"
          }
        },
        "ea3d0ebefd2d47b8bf9baede6f70c945": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "774c3e2d9dac4d2b8f73cb0298aa34c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb8d88ef1ea1469a85a2b4871cee7825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c380c2b1e1e54f93a6778b5ed4f65912": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af5867865649400ea97c3c809b988d90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bffb06e55ccb4126abeefad137ebf296": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "247d83557bf54318ae060f5cd0465ab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8bdb969c8c0949dfbcc2b763a5ebfe50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd2dbc8a270a42dba5ace810c017af36",
              "IPY_MODEL_5d47924e8e6d463d8a2405bcf6a33841",
              "IPY_MODEL_43c4feedf44c46f1a03cc9fe44abfe86"
            ],
            "layout": "IPY_MODEL_d5de5ede62024c448d49cd5e02eaed67"
          }
        },
        "fd2dbc8a270a42dba5ace810c017af36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_530ce5aba79a4573a3908ed470ab4802",
            "placeholder": "​",
            "style": "IPY_MODEL_70cb9a12fbf540c8a5312cae8f959cdb",
            "value": "model-00002-of-00003.safetensors: 100%"
          }
        },
        "5d47924e8e6d463d8a2405bcf6a33841": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6a92eb29cea4766ad2310f7fb2852f4",
            "max": 4999819336,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_794cbbb956694af5b5b389aa7115c756",
            "value": 4999819336
          }
        },
        "43c4feedf44c46f1a03cc9fe44abfe86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59eaaca5beef4c52a1125b462415965c",
            "placeholder": "​",
            "style": "IPY_MODEL_209cb4c5e6a94d8098da68fae5293a4f",
            "value": " 5.00G/5.00G [00:34&lt;00:00, 212MB/s]"
          }
        },
        "d5de5ede62024c448d49cd5e02eaed67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "530ce5aba79a4573a3908ed470ab4802": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70cb9a12fbf540c8a5312cae8f959cdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6a92eb29cea4766ad2310f7fb2852f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "794cbbb956694af5b5b389aa7115c756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "59eaaca5beef4c52a1125b462415965c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "209cb4c5e6a94d8098da68fae5293a4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea34c89fcdfb4855a5e15de54195d2e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54485f21648744aeb83a17bb7868eb3e",
              "IPY_MODEL_ea251c36c9994e82a33dcf7b03d546c2",
              "IPY_MODEL_97d8f8c5640349eab3e84286d5fe9015"
            ],
            "layout": "IPY_MODEL_3d6530efbe1e44c5bc2f64040dae79d7"
          }
        },
        "54485f21648744aeb83a17bb7868eb3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18d7c0dd6e6f44b58e97763954f2d064",
            "placeholder": "​",
            "style": "IPY_MODEL_07a03728272c400fb76140bb8c1c6004",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ea251c36c9994e82a33dcf7b03d546c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b233e9d88a3443b0b034790405e381d3",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9136ae52fb8c4cbe9e1a3aa3656649e6",
            "value": 3
          }
        },
        "97d8f8c5640349eab3e84286d5fe9015": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_644055f2b0e54c0e88c6094c8b3801e0",
            "placeholder": "​",
            "style": "IPY_MODEL_75ae8044b8bf4a67b956c842594f01cd",
            "value": " 3/3 [00:17&lt;00:00,  5.61s/it]"
          }
        },
        "3d6530efbe1e44c5bc2f64040dae79d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18d7c0dd6e6f44b58e97763954f2d064": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07a03728272c400fb76140bb8c1c6004": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b233e9d88a3443b0b034790405e381d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9136ae52fb8c4cbe9e1a3aa3656649e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "644055f2b0e54c0e88c6094c8b3801e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75ae8044b8bf4a67b956c842594f01cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8701403657ab4f2aa8f649f89307686d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0a341258e284bf0be8fa0f1fec951d7",
              "IPY_MODEL_f37b246b0f3a4be1bd0c01110f28b3f4",
              "IPY_MODEL_353b991b28f74de8b729f9bb432dcbc5"
            ],
            "layout": "IPY_MODEL_3a8429d34b254ebaa6be619bde843723"
          }
        },
        "f0a341258e284bf0be8fa0f1fec951d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c35fb76b83724d3eba35e242668c4a42",
            "placeholder": "​",
            "style": "IPY_MODEL_6f30f3f674964ae6867039eeed8a2a7b",
            "value": "generation_config.json: 100%"
          }
        },
        "f37b246b0f3a4be1bd0c01110f28b3f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ae2600da0f846c286efe184bc650df2",
            "max": 111,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c7c7a82632344b948a95fba999eee7db",
            "value": 111
          }
        },
        "353b991b28f74de8b729f9bb432dcbc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48a1e50d08ed4c09adb5cd9e970353ed",
            "placeholder": "​",
            "style": "IPY_MODEL_4b8fc7b86344409e9b42b42fc915a25c",
            "value": " 111/111 [00:00&lt;00:00, 14.4kB/s]"
          }
        },
        "3a8429d34b254ebaa6be619bde843723": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c35fb76b83724d3eba35e242668c4a42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f30f3f674964ae6867039eeed8a2a7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ae2600da0f846c286efe184bc650df2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7c7a82632344b948a95fba999eee7db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48a1e50d08ed4c09adb5cd9e970353ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b8fc7b86344409e9b42b42fc915a25c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 📚 Project Overview\n",
        "\n",
        "This project fine-tunes a large language model (Mistral-7B-Instruct-v0.2) to generate high-quality Statements of Purpose (SOPs) in a specific author's writing style.\n",
        "\n",
        "In this notebook, we cover the **complete workflow**:\n",
        "\n",
        "### 1. Data Preparation\n",
        "- **Load and clean the dataset** (`manasa_cleaned_file.jsonl`) containing SOPs formatted with clear instruction-style prompts.\n",
        "- **Remove reasoning sections** to create a pure SOP dataset for focused fine-tuning.\n",
        "\n",
        "### 2. Fine-Tuning Mistral-7B with LoRA (Low-Rank Adaptation)\n",
        "- **Quantize** the base model to 4-bit precision for memory-efficient training on an A100 GPU.\n",
        "- **Apply LoRA adapters** to fine-tune the model efficiently without updating all model weights.\n",
        "- **Fine-tune** the model on the custom SOP instruction dataset for 2 epochs.\n",
        "- **Log training metrics** and checkpoint intermediate model states.\n",
        "\n",
        "### 3. Model Inference and Evaluation\n",
        "- **Run inference** using the fine-tuned model on new prompts (e.g., SOPs for different programs).\n",
        "- **Verify** that the model generates complete, coherent SOPs aligned with the target writing style.\n",
        "\n",
        "✅ **End Result**:\n",
        "- A fine-tuned model saved to `/content/drive/MyDrive/mistral_sop_finetuned` that can generate SOPs based on new instructions.\n",
        "- Instructions provided for **how to load and use** the fine-tuned model after training.\n",
        "\n",
        "🔵 **Big Picture**:\n",
        "This fine-tuned SOP generation model lays the foundation for a larger project where we aim to also generate **stylistic reasoning explanations** alongside SOPs (in Model 2)."
      ],
      "metadata": {
        "id": "_RHc1W723tjp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCx3bSjOB1IF",
        "outputId": "5c17b1a8-550b-4bb7-b41b-5e29d0865028",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting trl\n",
            "  Downloading trl-0.16.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.16.1-py3-none-any.whl (336 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, bitsandbytes, trl\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.45.5 datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 trl-0.16.1 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shRnDe26B3J8",
        "outputId": "c292c3e5-6963-4665-d286-a614a5860ff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "30fcdfe95d42446fb319571e91a80708",
            "098ee563c896466b8667ee6161379770",
            "b2adbc8bd50744f58eb60e9a6699554b",
            "a5b57670b91e4a9e9baf162292678e5a",
            "4005046b521846459e567803b27b4e12",
            "415b1778e87e4a5891958b3a9ae82ab7",
            "a03a051c4b454e6480fd3eefacdf64c7",
            "d50aaf5cbf594dd2aaf360fa21001381",
            "72a6c4d06bb4430ea5be9f9ba60ada75",
            "62a9799066d34c43b0b15556b6ba56ee",
            "eef8f4cdb97043869ae6c8e9ebca206c",
            "86676ca5b48a44f8bf4b13773ffb1fb3",
            "3597e04d0f0346deb50ec51832bba5d8",
            "67c5457606014e3faca3a17ca06c1e4d",
            "40d56a88db974d689f51d8976267b4cf",
            "ecbf4ffd4d2f4c418e9f0e4de74f92c6",
            "711184e138d3405c9efcdd0ab4028e93",
            "8ab7e0bd1432451787fe949d0c83efca",
            "6d762a0ad00d4039ac74bcb2933ca337",
            "89717355cdef4a9d86e03274b8138366"
          ]
        },
        "id": "LCSo6fO2B5nD",
        "outputId": "dfba02c0-ee95-4f70-f116-35b440be0867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30fcdfe95d42446fb319571e91a80708"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧹 Remove [REASONING] Sections from Dataset\n",
        "\n",
        "In this step:\n",
        "- We load the formatted dataset (`manasa_sop_formatted_updated.jsonl`) where each entry contains both an SOP and a reasoning explanation.\n",
        "- We use a regular expression (`regex`) to **remove everything between `[REASONING]` and `[/REASONING]`** in each sample.\n",
        "- This leaves only the clean SOP content in the `\"text\"` field.\n",
        "- The cleaned dataset is saved as `/content/drive/MyDrive/manasa_cleaned_file.jsonl`.\n",
        "\n",
        "✅ This prepares a pure SOP-only dataset, useful if we want to fine-tune a model solely on SOP generation without explanations."
      ],
      "metadata": {
        "id": "gO6ZUFFK3YdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "input_path = \"/content/drive/MyDrive/manasa_sop_formatted_updated.jsonl\"\n",
        "output_path = \"/content/drive/MyDrive/manasa_cleaned_file.jsonl\"\n",
        "\n",
        "def remove_reasoning(text):\n",
        "    # Use regex to remove everything between [REASONING] and [/REASONING]\n",
        "    return re.sub(r\"\\[REASONING\\](.|\\n)*?\\[/REASONING\\]\", \"\", text).strip()\n",
        "\n",
        "with open(input_path, 'r') as infile, open(output_path, 'w') as outfile:\n",
        "    for line in infile:\n",
        "        sample = json.loads(line)\n",
        "        cleaned_text = remove_reasoning(sample[\"text\"])\n",
        "        sample[\"text\"] = cleaned_text\n",
        "        json.dump(sample, outfile)\n",
        "        outfile.write(\"\\n\")\n",
        "\n",
        "print(\"✅ All [REASONING] sections removed and cleaned data saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEbRjcLZCARM",
        "outputId": "a089a7ac-3ee1-481b-d455-740c09dd0b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All [REASONING] sections removed and cleaned data saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📚 Fine-Tuning Mistral-7B-Instruct-v0.2 for SOP Generation — Full Pipeline\n",
        "\n",
        "This code fine-tunes the Mistral-7B-Instruct-v0.2 model to generate Statements of Purpose (SOPs) in a specific writing style using custom instruction-formatted data.\n",
        "\n",
        "### Main steps:\n",
        "\n",
        "1. **Install Required Libraries**\n",
        "   - Install Hugging Face Transformers, Datasets, PEFT (for LoRA), Bitsandbytes (for 4-bit quantization), and W&B (optional logging).\n",
        "\n",
        "2. **Set Up Environment**\n",
        "   - Mount Google Drive to access your dataset and save outputs.\n",
        "   - Define file paths for loading data and saving models.\n",
        "\n",
        "3. **Load and Prepare Dataset**\n",
        "   - Load the SOP dataset from a `.jsonl` file.\n",
        "   - Convert it into a Hugging Face `Dataset` object.\n",
        "   - Split the data into 90% training and 10% validation.\n",
        "\n",
        "4. **Load Tokenizer**\n",
        "   - Load the tokenizer from the Mistral-7B-Instruct model.\n",
        "   - Set padding token to the EOS token and prepare for right-side padding.\n",
        "\n",
        "5. **Tokenize the Dataset**\n",
        "   - Tokenize the SOP text examples.\n",
        "   - Set a maximum length (2048 tokens) to fit long SOPs.\n",
        "\n",
        "6. **Load the Model with 4-bit Quantization**\n",
        "   - Load Mistral-7B in 4-bit NF4 quantized format for efficient fine-tuning on limited GPU memory (A100).\n",
        "   - Automatically map model layers to GPU using `device_map=\"auto\"`.\n",
        "\n",
        "7. **Apply LoRA (Parameter-Efficient Fine-Tuning)**\n",
        "   - Configure a LoRA setup targeting specific projection layers (`q_proj`, `k_proj`, etc.).\n",
        "   - Inject LoRA adapters into the model to fine-tune a small number of parameters efficiently.\n",
        "\n",
        "8. **Configure Training**\n",
        "   - Define training hyperparameters using `TrainingArguments`, including:\n",
        "     - Batch sizes, learning rate, gradient accumulation, save steps, evaluation steps, mixed precision (fp16), etc.\n",
        "   - Set W&B as the logging platform if needed.\n",
        "\n",
        "9. **Initialize Trainer**\n",
        "   - Wrap the model, training arguments, datasets, and data collator using Hugging Face `Trainer`.\n",
        "\n",
        "10. **Fine-Tune the Model**\n",
        "    - Start the fine-tuning process with `trainer.train()`.\n",
        "    - Save model checkpoints during training and the final model after training.\n",
        "\n",
        "11. **Inference and Testing**\n",
        "    - Define a `generate_sop` function to test the fine-tuned model.\n",
        "    - Provide a prompt and generate a complete SOP.\n",
        "    - Print an example SOP generated for a Data Science Master's degree at MIT.\n",
        "\n",
        "12. **Post-Training Usage Instructions**\n",
        "    - Simple steps for reloading and using the fine-tuned model later."
      ],
      "metadata": {
        "id": "NCm4kfUM3kwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning Mistral-7B-Instruct-v0.2 for SOP Generation\n",
        "# This code assumes you're running in Google Colab with an A100 GPU\n",
        "\n",
        "# Install required libraries\n",
        "!pip install -q transformers datasets accelerate peft bitsandbytes wandb sentencepiece\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import random\n",
        "from google.colab import drive\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "\n",
        "# Mount Google Drive to access your data\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "jsonl_path = '/content/drive/MyDrive/manasa_cleaned_file.jsonl'  # Update this to your JSONL file path\n",
        "output_dir = '/content/drive/MyDrive/mistral_sop_finetuned'\n",
        "\n",
        "# Load dataset from JSONL\n",
        "def load_jsonl_data(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "sop_data = load_jsonl_data(jsonl_path)\n",
        "\n",
        "\n",
        "# Create HF Dataset\n",
        "dataset = Dataset.from_list(sop_data)\n",
        "\n",
        "# Split the dataset into training and validation\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = dataset[\"train\"]\n",
        "eval_dataset = dataset[\"test\"]\n",
        "\n",
        "print(f\"Training examples: {len(train_dataset)}\")\n",
        "print(f\"Validation examples: {len(eval_dataset)}\")\n",
        "\n",
        "# Print a sample for verification\n",
        "print(\"\\nSample formatted prompt:\")\n",
        "print(train_dataset[0][\"text\"])\n",
        "\n",
        "# Load tokenizer and prepare for training\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Function to tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=2048,  # Adjust based on your SOP lengths\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_train = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "tokenized_val = eval_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "# Check if tokenization was successful\n",
        "print(f\"\\nTokenized train dataset length: {len(tokenized_train)}\")\n",
        "print(f\"Tokenized validation dataset length: {len(tokenized_val)}\")\n",
        "\n",
        "# Configure quantization for efficient fine-tuning on A100\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load the model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Prepare for PEFT/LoRA fine-tuning\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=16,                     # Rank dimension\n",
        "    lora_alpha=32,            # Alpha parameter for LoRA scaling\n",
        "    lora_dropout=0.05,        # Dropout probability for LoRA layers\n",
        "    bias=\"none\",              # We're not training biases\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(f\"Trainable parameters: {model.print_trainable_parameters()}\")\n",
        "\n",
        "# Set up training arguments\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    num_train_epochs=2,          # Start with a small number of epochs to prevent overfitting\n",
        "    per_device_train_batch_size=2,  # Adjust based on GPU memory\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=50,\n",
        "    logging_steps=10,\n",
        "    fp16=True,\n",
        "    push_to_hub=False,\n",
        "    save_total_limit=3,\n",
        "    report_to=\"wandb\",  # Set to \"wandb\" if you want to use Weights & Biases\n",
        ")\n",
        "\n",
        "# Create data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # We're not doing masked language modeling\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Start fine-tuning\n",
        "print(\"Starting fine-tuning process...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"Model saved to {output_dir}\")\n",
        "\n",
        "# Function to test the model\n",
        "def generate_sop(prompt, max_length=2048):\n",
        "    input_text = f\"<s>[INST] {prompt} [/INST]\\n\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate text\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=0.7,\n",
        "            top_p=0.85,\n",
        "            do_sample=True,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    # Extract just the response part\n",
        "    response = generated_text.split(\"[/INST]\")[1].strip()\n",
        "    return response\n",
        "\n",
        "# Test the model with a sample prompt\n",
        "test_prompt = \"Write me an SOP for pursuing a Master's Degree in Data Science at MIT.\"\n",
        "print(\"\\nTesting the fine-tuned model with a sample prompt:\")\n",
        "generated_sop = generate_sop(test_prompt)\n",
        "print(generated_sop)\n",
        "\n",
        "# Instructions for using the model after this session\n",
        "print(\"\\n==== How to use the fine-tuned model ====\")\n",
        "print(\"1. Load the saved model from your Google Drive\")\n",
        "print(\"2. Use the 'generate_sop' function with your desired prompt\")\n",
        "print(\"3. You can adjust temperature and top_p for more/less creativity\")\n",
        "print(\"4. For new domains not in training data, the model will generate SOPs in the same style\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7bd8438bd7e441c08202640bf83e1140",
            "1760f7189b3941e48e636d11345033f6",
            "a67de702761e41cd88b7f279850b4f8f",
            "18f9d8baf861487d868d8b49a6ad3d9d",
            "7efe0fbcd6a245d5b93f32308d321e67",
            "d71b4aca1aaa4cbe99c90189d0891058",
            "85d28fe8de554eba84ec230397d0c03f",
            "748141bfdd3b42a9b219c3b4b982ea6c",
            "dce0e50f1a9f4a36a6f77de5ebe5370e",
            "faed3e7bb2df48c69d1e4c54a17a9d05",
            "9f109737b2844f9e97db76679689927f",
            "c2291fda5d7c4281ac0a35ae067ab1bf",
            "d8c129df57cd4b78bd48eec1f69caf6a",
            "dc66e629cc934b969e3bfb7466619315",
            "cd8496092c384a3db8b84340be839fde",
            "b54b020120bf4d98b19ebe76abb8b74d",
            "c23fdc4c13ea4ccf9f31d498c6b14f18",
            "4517819dfe4a424db1fc4e6bb1a534fe",
            "258022a102a6408a92f3070cfff11772",
            "7d3ded2e50414642917be3a8ecee84ec",
            "aaa9d831e0e947ae832a2f3c65f1d3cc",
            "c70f180037444f849b3af9e845ead688",
            "058267e978dc4fbfb2f4115f5b771102",
            "4b32596174f24232bdb429e1dcc71986",
            "7d27b9fa37234934a6739669fc27d53b",
            "d6fcf9c22fe642b28ec121ee49008c4c",
            "a5d23f1d45234e30aa62d43dca10e683",
            "536d64bbae2d46f08b5ec012fd5831b2",
            "023f7b4e14fb4adda6a81bd90cb04c83",
            "114b5bdba51f4b098b9e3367da6e410f",
            "a52e722d23684553a22a4cff327b1568",
            "980933b32a8c43e3aaeefd2eab6108a6",
            "f2f8718ce8b8418f89cf87148a36a254"
          ]
        },
        "id": "lwb8FIJxIdF4",
        "outputId": "d81cbd66-f5f5-4d6b-ea37-1d88c94af3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Training examples: 503\n",
            "Validation examples: 56\n",
            "\n",
            "Sample formatted prompt:\n",
            "<s>[INST] Write me an SOP for pursuing a Master's Degree in Computer Science, focusing on Artificial Intelligence and Machine Learning. [/INST]\n",
            "[SOP]\n",
            "STATEMENT OF PURPOSE The power of knowing a system inside out, inclusive of its hardware and software functionalities, will not only revise and refurbish the entire purpose of its creation but also incredibly increase the scope of its implementation. With growing technology, advancement of science and learning, I developed an impeccable interest for computers, their languages and their highly appreciated scope of advancement. The world is constantly evolving and looking deeply through the lens of development, one can certainly say that computers have a role dedicated to lifting the universal veil of information exchange, therefore uplifting the geographic perspective and therefore, revising the supremacy of a nation. Coming from an electronic background, seeing the need for advancement in processes and procedures and studying progress and methods of implementing it, I can say with confidence that growing towards a sphere of computer science will strongly enrich and second my ability for exploring the mighty areas of technology and its globalisation while understanding industrialisation. As a young kid, I was greatly swept away by the realm of gadgets, electronics around me. I was very inquisitive by nature, explorative in mind and experimental in learning. In school, mathematics and science entertained and inspired me the most given their competitive content and problemsolving requisites. It required out of the box thinking and logic to understand, solve, and compete. As a 6th grader, I won a gold in the International Math Olympiad, I continued to win several medals and certificates in this ring. I also participated in the Ramanujan examination and excelled at it as the topper in the 7th grade. I was extremely driven by subjects that demanded morethanwhatsontextbook thinking. I also undertook several IIT (Indian Institute of Technology) examinations and stood victorious while topping many of them. Besides books, I also shared a peculiar interest for dramas and dancing, cricket and football, film making and theatrical performances. As a viewer and a player, being associated with them helped me develop an overall ability to comprehend, communicate and learn through perspectives besides mine. I also had the incredible opportunity to serve as the head boy of school given my variety of skills and abilities I brought to the table. Since I was swept away with the idea of understanding how things worked, I felt a strong desire to study the theory that went behind it. And therefore, I decided to pursue mathematics, physics and chemistry as my core during intermediate. Doing well with them would give me a ticket to study EEE (Electronics and Electrical Engineering) as my undergraduate degree. Therefore, I worked hard to scored a 93.8 which landed me in one of the best and prestigious institutions of India. Impeccably blessed with a vast possibility of advancing myself in every way from personal to professional life, I made it a point to utilise every opportunity that came my way and every resource I could get my hands on. To achieve the best I could with studies, while staying on top of the syllabus, I also read numerous journals, research papers and supplementary materials. I gained impressive knowledge in subjects such as XXX which helped me build my skills around XXX industrially. I have an outstanding cumulative of 7.65 CGPA so far with respect to the competition that grows within the campus far after day. Considering my grades and wholesome approach to managing, I was elected as the class representative of the student council during the second and third years of engineering. I also served as the executive member of the NSS club where I took particular interest in visiting villages and sharing expertise in getting their hands around digital advancements. I believed that knowing what a digital world would require to communicate lifestyle, would always help keep you and your community afloat. Besides academics, I also took part in an exquisite association the institute fostered and served as the joint secretary. I supervised the community, giving out ideas and methods to creatively manage and lead while also maintaining the treasury of the department. While at it, I got my hands on a computer and the interest to explore more and more only grew. I soon collaborated with students from computer science department to exchange information and learnings, and there began my journey towards computer science engineering. Slowly, after undertaking several courses in XXX and certifications around XXX, I gained the necessary knowledge and expertise towards building realtime projects based on computers and technology. I developed a project based on chat room web application with reply suggestions along with a team member which allowed people to enter chat rooms and have a real time conversation. I made use of XXX to build the application and run it seamlessly. Second, I also developed a data base management system around railway reservation system, I was focused on learning and implementing the theoretical aspects of DBMS and therefore worked on developing a real time project using XXX. And third, I worked on bringing together a stock price predictor based on data science using Python and Google collaborator. The inspiration behind this was an incident that took place in India when all hell broke lose around Paytm when the founder faced incredible looses with relation to the trade market. My strengths therefore include a strong understanding of C, Python, Java Script, HTML, CSS, SQL, Frameworks, Django and WebDev besides electrical subjects and insights. Sharing a strong desire to pursue computer science and succeed in the field, I worked extremely hard to balance my time for both my electrical engineering and computer engineering. I not only explored the concepts theoretically but also worked on understanding the practicalities and problems involved. And I constantly strived and thrived at upgrading myself in it. Today, I believe the right step into my future would be pursuing a masters degree in computer science from your university of XXX. The university of xxx and the USA together will help me incredibly grow my knowledge, expertise and experience given the vast network, exposure and advancement that the nation fosters within it. I therefore look forward for studying and growing with your guidance in near future.\n",
            "[/SOP]\n",
            "</s>\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bd8438bd7e441c08202640bf83e1140",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/503 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2291fda5d7c4281ac0a35ae067ab1bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/56 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tokenized train dataset length: 503\n",
            "Tokenized validation dataset length: 56\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "058267e978dc4fbfb2f4115f5b771102",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 41,943,040 || all params: 7,283,675,136 || trainable%: 0.5758\n",
            "Trainable parameters: None\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting fine-tuning process...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlasyaedunuri\u001b[0m (\u001b[33mlasyaedunuri-university-of-north-carolina-at-charlotte\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250424_050557-87i17lb2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lasyaedunuri-university-of-north-carolina-at-charlotte/huggingface/runs/87i17lb2' target=\"_blank\">/content/drive/MyDrive/mistral_sop_finetuned</a></strong> to <a href='https://wandb.ai/lasyaedunuri-university-of-north-carolina-at-charlotte/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/lasyaedunuri-university-of-north-carolina-at-charlotte/huggingface' target=\"_blank\">https://wandb.ai/lasyaedunuri-university-of-north-carolina-at-charlotte/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/lasyaedunuri-university-of-north-carolina-at-charlotte/huggingface/runs/87i17lb2' target=\"_blank\">https://wandb.ai/lasyaedunuri-university-of-north-carolina-at-charlotte/huggingface/runs/87i17lb2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [126/126 19:02, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.260900</td>\n",
              "      <td>2.452541</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to /content/drive/MyDrive/mistral_sop_finetuned\n",
            "\n",
            "Testing the fine-tuned model with a sample prompt:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SOP]\n",
            "STATEMENT OF PURPOSE \"I believe, the most powerful thing one can possess is the ability to predict, to imagine, to visualise and to foresee.\" Hailing from a small town, I have seen my father run a grocery store and my mother work as a school teacher. My parents always supported me to do what I wanted to do and never pressured me into choosing a specific stream of study. I was a very good student at school, always topped the class, was the topper of the school in 10th grade, and was very active in extracurricular activities. I participated in many state and districtlevel competitions and won several medals. I was also a very good athlete, I won many gold medals in athletics. I was also the school caption for three consecutive years. I was a very good student, always had a positive attitude towards life. I always believed that there is always more to learn and explore, therefore I never settled down with what I had. I was a very good student, always topped the class, was the topper of the school in 10th grade, and was very active in extracurricular activities. I participated in many state and districtlevel competitions and won several medals. I was also a very good athlete, I won many gold medals in athletics. I was also the school caption for three consecutive years. I was a very good student, always had a positive attitude towards life. I always believed that there is always more to learn and explore, therefore I never settled down with what I had. I was very hardworking and determined. I always wanted to make a good career for myself in the future. I wanted to study computers and therefore, I studied mathematics, physics and chemistry during intermediate. I scored very well in the examinations and joined a good engineering college. I joined the computer science department to pursue my dreams. I was very excited to study computers and the field of technology. I enjoyed studying the subjects of programming languages, computer networks, data structures, database management systems and more. I also attended several workshops and seminars conducted by the college. I was very interested in learning new things, therefore I attended many workshops and seminars on the subjects of artificial intelligence, machine learning and cloud computing. I also completed several projects, one of them being the development of a web application for a real estate company. I also developed a website for a famous restaurant in India, The Spice Villa. I also completed a project on the application of artificial intelligence in the field of healthcare. I also published several research papers in international journals. I also worked on several projects as part of my academic curriculum. I also worked on several projects as part of my internship program. I interned at the prestigious Indian Institute of Technology, Chennai, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Madras, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Delhi, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Mumbai, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Kanpur, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Roorkee, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Guwahati, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Hyderabad, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Indore, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Bhubaneswar, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Patna, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Gandhinagar, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Jodhpur, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Kharagpur, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Goa, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Thiruvananthapuram, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Varanasi, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Bhabha Atomic Research Centre, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Tata Institute of Fundamental Research, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Bharat Electronics Limited, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, National Aerospace Laboratories, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Indian Space Research Organisation, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, National Institution of Technology, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Indian Institute of Science Education and Research, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Indira Gandhi Centre for Atomic Research, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, Vikram Sarabhai South Indian Institute of Technology, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, National Institute of Technology, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, National Institute of Technology, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, National Institute of Technology, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, National Institute of Technology, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, National Institute of Technology, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, National Institute of Technology, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, National Institute of Technology, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, National Institute of Technology, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, National Institute of Technology, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, National Institute of Technology, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, National Institute of Technology, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, National Institute of Technology, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, National Institute of Technology, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, National Institute of Technology, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, National Institute of Technology, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, National Institute of Technology, where I worked on developing a web application for managing the schedules of students. I also interned at the famous Indian Institute of Technology, National Institute of Technology, where I worked on developing a web application for managing the schedules of students. I also interned at the\n",
            "\n",
            "==== How to use the fine-tuned model ====\n",
            "1. Load the saved model from your Google Drive\n",
            "2. Use the 'generate_sop' function with your desired prompt\n",
            "3. You can adjust temperature and top_p for more/less creativity\n",
            "4. For new domains not in training data, the model will generate SOPs in the same style\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧪 Test the Fine-Tuned Model with a New Prompt\n",
        "\n",
        "In this step:\n",
        "- We test the fine-tuned Mistral-7B model by providing a new instruction prompt:\n",
        "  - \"Write me an SOP for pursuing a Master's Degree in Political Science from Duke University.\"\n",
        "- The `generate_sop` function:\n",
        "  - Formats the prompt using `[INST]...[/INST]` tags.\n",
        "  - Tokenizes the input and moves it to the GPU.\n",
        "  - Generates a response using the model with sampling settings (`temperature=0.7`, `top_p=0.85`).\n",
        "  - Extracts and prints the generated SOP text.\n",
        "\n",
        "✅ This helps verify that the model can now generate full SOPs for unseen prompts in a style consistent with the training data."
      ],
      "metadata": {
        "id": "fybJ6uXH4HRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model with a sample prompt\n",
        "test_prompt = \"Write me an SOP for pursuing a Master's Degree in Political Science from Duke University.\"\n",
        "print(\"\\nTesting the fine-tuned model with a sample prompt:\")\n",
        "generated_sop = generate_sop(test_prompt)\n",
        "print(generated_sop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFA1L3oeSVNX",
        "outputId": "a49d2e96-8d64-4eb3-a09b-d8c084477354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing the fine-tuned model with a sample prompt:\n",
            "[SOP]\n",
            "STATEMENT OF PURPOSE \"The world today is a global village, interconnected by various factors that influence one another. The power of politics is one such factor that influences the growth of the world. The world is intertwined by politics and the progression of technology. The two have always influenced one another, where politics influence technology and technology, in turn, has helped politics to grow. As a young girl, I enjoyed the concept of politics and the role it played in shaping the world. The power of politics was always evident to me. It was everywhere, from the small scale to the big scale, and I was intrigued by its influence on the world. I enjoyed participating in political discussions and debates. I enjoyed reading books, watching films and listening to music that had political undertones. I was interested in understanding the political landscape of the world. I was also interested in understanding the role of women in the political sphere. I wanted to understand why there were so few women in the political sphere. I wanted to understand the reasons behind it and how we could ensure more women were given the opportunity to participate in politics. I was a student of St. Xaviers School, where I studied under the CBSE curriculum. The curriculum helped me develop a strong foundation in understanding politics and its various facets. I was a bright student, often participating in various cultural and literary activities. I enjoyed taking part in debates and discussions, especially around topics that allowed me to express my viewpoints. I was also a good student academically. I was a diligent student, always striving to do better. I completed school with a 9.5 CGPA and higher secondary education in the stream of mathematics, physics and chemistry with 94. I was determined to pursue the stream of my choice and study political science. I was thrilled to have the opportunity to study the subjects of my choice and learn more about politics. I completed my undergraduation with a Bachelor of Arts in Political Science from Christ University. I was thrilled to have the opportunity to study the subjects that interested me the most. I studied the subjects of International Relations, Political Theory, Indian Political Thought, Public Policy and Governance, Political Economy and more. These subjects helped me develop a strong understanding of the field of politics and the various facets of it. I also learned about the Indian political system, its structure and functioning. I was fascinated by the concept of power and how it influenced the functioning of the government. I was also interested in understanding the role of women in politics and why there were so few women in the political sphere. I wrote an essay on the topic \"The role of women in politics\" as part of the curriculum, where I spoke about how women are underrepresented in politics despite having the right to vote and participate. I also studied the political climate of India and how the government has evolved over time. I also studied the political landscape of other countries, including the USA, UK, China and more. I also learned about the role of technology in politics and how it has influenced the way politics are conducted today. I also studied the role of social media in politics and how it has influenced the voter turnout in various elections. I was also interested in studying the role of technology in politics, especially with the rise of artificial intelligence and machine learning. I also studied the role of big data in politics and how it has influenced the voter turnout in various elections. I also learned about the role of technology in politics and how it has influenced the way politics are conducted today. I also studied the role of social media in politics and how it has influenced the voter turnout in various elections. I was also interested in studying the role of big data in politics and how it has influenced the voter turnout in various elections. I also studied the role of artificial intelligence and machine learning in politics and how it has influenced the way politics are conducted today. I also studied the role of technology in politics and how it has influenced the way politics are conducted today. I also studied the role of artificial intelligence and machine learning in politics and how it has influenced the way politics are conducted today. I also studied the role of big data in politics and how it has influenced the voter turnout in various elections. I also studied the role of technology in politics and how it has influenced the way politics are conducted today. I also studied the role of artificial intelligence and machine learning in politics and how it has influenced the way politics are conducted today. I also studied the role of big data in politics and how it has influenced the voter turnout in various elections. I also studied the role of technology in politics and how it has influenced the way politics are conducted today. I also studied the role of artificial intelligence and machine learning in politics and how it has influenced the way politics are conducted today. I also studied the role of big data in politics and how it has influenced the voter turnout in various elections. I also studied the role of technology in politics and how it has influenced the way politics are conducted today. I also studied the role of artificial intelligence and machine learning in politics and how it has influenced the way politics are conducted today. I also studied the role of big data in politics and how it has influenced the voter turnout in various elections. I also studied the role of technology in politics and how it has influenced the way politics are conducted today. I also studied the role of artificial intelligence and machine learning in politics and how it has influenced the way politics are conducted today. I also studied the role of big data in politics and how it has influenced the voter turnout in various elections. I also studied the role of technology in politics and how it has influenced the way politics are conducted today. I also studied the role of artificial intelligence and machine learning in politics and how it has influenced the way politics are conducted today. I also studied the role of big data in politics and how it has influenced the voter turnout in various elections. I also studied the role of technology in politics and how it has influenced the way politics are conducted today. I also studied the role of artificial intelligence and machine learning in politics and how it has influenced the way politics are conducted today. I also studied the role of big data in politics and how it has influenced the voter turnout in various elections. I also studied the role of technology in politics and how it has influenced the way politics are conducted today. I also studied the role of artificial intelligence and machine learning in politics and how it has influenced the way politics are conducted today. I also studied the role of big data in politics and how it has influenced the voter turnout in various elections. I also studied the role of technology in politics and how it has influenced the way politics are conducted today. I also studied the role of artificial intelligence and machine learning in politics and how it has influenced the way politics are conducted today. I also studied the role of big data in politics and how it has influenced the voter turnout in various elections. I also studied the role of technology in politics and how it has influenced the way politics are conducted today. I also studied the role of artificial intelligence and machine learning in politics and how it has influenced the way politics are conducted today. I also studied the role of big data in politics and how it has influenced the voter turnout in various elections. I also studied the role of technology in politics and how it has influenced the way politics are conducted today. I also studied the role of artificial intelligence and machine learning in politics and how it has influenced the way politics are conducted today. I also studied the role of big data in politics and how it has influenced the voter turnout in various elections. I also studied the role of technology in politics and how it has influenced the way politics are conducted today. I also studied the role of artificial intelligence and machine learning in politics and how it has influenced the way politics are conducted today. I also studied the role of big data in politics and how it has influenced the voter turnout in various elections. I also studied the role of technology in politics and how it has influenced the way politics are conducted today. I also studied the role of artificial intelligence and machine learning in politics and how it has influenced the way politics are conducted today. I also studied the role of big data in politics and how it has influenced the voter turnout in various elections. I also studied the role of technology in politics and how it has influenced the way politics are conducted today. I also studied the role of artificial intelligence and machine learning in politics and how it has influenced the way politics are conducted today. I also studied the role of big data in politics and how it has influenced the voter turnout in various elections. I also studied the role of technology in politics and how it has influenced the way politics are conducted today. I also studied the role of artificial intelligence and machine learning in politics and how it has influenced the way politics are conducted today. I also studied the role of big data in politics and how it has influenced the voter turnout in various elections. I also studied the role of technology in politics and how it has influenced the way politics are conducted today. I also studied the role of artificial intelligence and machine learning in politics and how it has influenced the way politics are conducted today. I also studied the role of big data in politics and how it has influenced the voter turnout in various elections. I also studied the role of technology in politics and how it has influenced the way politics are conducted today. I also studied the role of artificial intelligence and machine learning in politics and how it has influenced the way politics are conducted today. I also studied the role of big data in politics and how it has influenced the voter turnout in various elections. I also studied the role of technology in politics and how it has\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsQvtUNoUMI2",
        "outputId": "cf69ce90-2af7-43f6-ff84-667d0eb22580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=34741cbfaf713ae30f24c2ad0584c0bd42d644710173c30d6133ee20caae9814\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📊 Model Evaluation Pipeline for Fine-Tuned SOP Generator\n",
        "\n",
        "This block evaluates the fine-tuned Mistral-7B-Instruct model for SOP generation.\n",
        "\n",
        "### What this script does:\n",
        "\n",
        "1. **Load Fine-Tuned Model**\n",
        "   - Load the model and tokenizer from the specified saved directory.\n",
        "   - Load in 4-bit quantized mode to save GPU memory.\n",
        "\n",
        "2. **SOP Generation**\n",
        "   - Given a test prompt, generate an SOP using the fine-tuned model.\n",
        "   - Automatically format the prompt according to the `[INST] ... [/INST]` format expected by Mistral.\n",
        "\n",
        "3. **Completeness Checking**\n",
        "   - Check if the generated SOP is **complete**:\n",
        "     - Ends with proper punctuation (`.`, `!`, `?`).\n",
        "     - Doesn't end mid-sentence or with dangling conjunctions (e.g., \"and\", \"but\").\n",
        "\n",
        "4. **Optional ROUGE Score Calculation**\n",
        "   - If reference SOPs are available, calculate **ROUGE-1**, **ROUGE-2**, and **ROUGE-L** scores to measure overlap between generated and reference SOPs.\n",
        "\n",
        "5. **Prompt Sampling**\n",
        "   - Two modes for testing:\n",
        "     - **Held-out prompts**: Use fresh prompts not seen during training (e.g., \"Write me an SOP for Machine Learning\").\n",
        "     - **Training data prompts**: Sample prompts and reference SOPs from the training dataset for evaluation.\n",
        "\n",
        "6. **Evaluation Metrics**\n",
        "   - For each generated SOP, log:\n",
        "     - The prompt\n",
        "     - The generated SOP\n",
        "     - Whether it is complete (boolean)\n",
        "     - ROUGE scores (if applicable)\n",
        "   - Calculate and report:\n",
        "     - Average completeness score across samples\n",
        "     - Average ROUGE scores (if references are available)\n",
        "\n",
        "7. **Save Results**\n",
        "   - Save detailed results and summary metrics into a `.json` file (`evaluation_results.json`).\n",
        "\n",
        "8. **Notebook and CLI Compatibility**\n",
        "   - If running inside a notebook, simulate command-line arguments.\n",
        "   - If running from the command line (`python evaluate.py`), accept `--model_path`, `--dataset`, etc. as arguments.\n",
        "\n",
        "---\n",
        "\n",
        "✅ **By the end of this evaluation script**:\n",
        "- You will have a detailed report showing how well your fine-tuned model generates SOPs:\n",
        "  - Are the SOPs complete and polished?\n",
        "  - How well do they match ground-truth SOPs (if available)?\n",
        "  - Metrics like Completeness % and ROUGE scores are computed."
      ],
      "metadata": {
        "id": "mnbR80hc4TqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import re\n",
        "import argparse\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from rouge_score import rouge_scorer\n",
        "import random\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def load_model_and_tokenizer(model_path):\n",
        "    \"\"\"Load the fine-tuned model and tokenizer\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "    # Load model in 4-bit to save memory\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        load_in_4bit=True,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_sop(model, tokenizer, prompt, max_length=2048):\n",
        "    \"\"\"Generate an SOP using the loaded model\"\"\"\n",
        "    # Format prompt according to Mistral's chat template\n",
        "    formatted_prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
        "\n",
        "    # Create generation pipeline\n",
        "    generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=max_length,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        repetition_penalty=1.15,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    # Generate text\n",
        "    generated = generator(formatted_prompt)[0]['generated_text']\n",
        "\n",
        "    # Extract SOP part\n",
        "    output = generated.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "    # Try to extract just the SOP content if it uses the [SOP] tags\n",
        "    sop_match = re.search(r'\\[SOP\\](.*?)(?:\\[\\/SOP\\]|$)', output, re.DOTALL)\n",
        "    if sop_match:\n",
        "        output = sop_match.group(1).strip()\n",
        "\n",
        "    return output\n",
        "\n",
        "def check_completeness(sop):\n",
        "    \"\"\"Check if an SOP is complete (doesn't end mid-sentence)\"\"\"\n",
        "    # Clean up the text\n",
        "    sop = sop.strip()\n",
        "\n",
        "    # Check last sentence\n",
        "    sentences = sent_tokenize(sop)\n",
        "    if not sentences:\n",
        "        return False\n",
        "\n",
        "    last_sentence = sentences[-1]\n",
        "\n",
        "    # Check for proper punctuation at the end\n",
        "    if not re.search(r'[.!?][\\s\"\\']*$', last_sentence):\n",
        "        return False\n",
        "\n",
        "    # Check for dangling words that suggest incomplete thought\n",
        "    incomplete_endings = [\n",
        "        \"and\", \"or\", \"but\", \"however\", \"therefore\", \"thus\",\n",
        "        \"moreover\", \"furthermore\", \"consequently\", \"since\", \"has\", \"a\"\n",
        "    ]\n",
        "\n",
        "    for word in incomplete_endings:\n",
        "        if last_sentence.lower().endswith(word.lower()):\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def evaluate_model(model_path, test_prompts, reference_sops=None):\n",
        "    \"\"\"Evaluate the model on test prompts\"\"\"\n",
        "    model, tokenizer = load_model_and_tokenizer(model_path)\n",
        "\n",
        "    results = []\n",
        "    completeness_scores = []\n",
        "\n",
        "    # Set up ROUGE scorer\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    # Process each test prompt\n",
        "    for i, prompt in enumerate(tqdm(test_prompts, desc=\"Generating SOPs\")):\n",
        "        # Generate SOP\n",
        "        generated_sop = generate_sop(model, tokenizer, prompt)\n",
        "\n",
        "        # Check completeness\n",
        "        is_complete = check_completeness(generated_sop)\n",
        "        completeness_scores.append(int(is_complete))\n",
        "\n",
        "        result = {\n",
        "            \"prompt\": prompt,\n",
        "            \"generated_sop\": generated_sop,\n",
        "            \"is_complete\": is_complete\n",
        "        }\n",
        "\n",
        "        # Calculate ROUGE scores if we have references\n",
        "        if reference_sops and i < len(reference_sops):\n",
        "            reference = reference_sops[i]\n",
        "            rouge_scores = scorer.score(reference, generated_sop)\n",
        "            result[\"rouge1\"] = rouge_scores[\"rouge1\"].fmeasure\n",
        "            result[\"rouge2\"] = rouge_scores[\"rouge2\"].fmeasure\n",
        "            result[\"rougeL\"] = rouge_scores[\"rougeL\"].fmeasure\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "    # Calculate overall completeness score\n",
        "    overall_completeness = sum(completeness_scores) / len(completeness_scores) if completeness_scores else 0\n",
        "\n",
        "    return results, overall_completeness\n",
        "\n",
        "def sample_test_prompts(dataset_path, num_samples=10, held_out=True):\n",
        "    \"\"\"Sample test prompts from the dataset\"\"\"\n",
        "    # Load dataset\n",
        "    dataset = []\n",
        "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                item = json.loads(line.strip())\n",
        "                dataset.append(item)\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "\n",
        "    if held_out:\n",
        "        # Create completely new prompts not in the dataset\n",
        "        courses = [\n",
        "            \"Machine Learning\", \"Business Analytics\", \"Digital Marketing\",\n",
        "            \"Public Health\", \"Quantum Computing\", \"Cybersecurity\",\n",
        "            \"Artificial Intelligence\", \"Urban Planning\", \"Robotics\",\n",
        "            \"Applied Mathematics\", \"Film Production\", \"Game Design\",\n",
        "            \"Environmental Law\", \"Political Science\", \"International Relations\"\n",
        "        ]\n",
        "\n",
        "        # Create prompts\n",
        "        return [f\"Write me an SOP for pursuing a Master's Degree in {course}.\" for course in\n",
        "                random.sample(courses, min(num_samples, len(courses)))]\n",
        "    else:\n",
        "        # Extract existing prompts from the dataset\n",
        "        prompts = []\n",
        "        references = []\n",
        "\n",
        "        for item in dataset:\n",
        "            # Check format\n",
        "            if \"text\" in item:\n",
        "                text = item[\"text\"]\n",
        "                instruction_match = re.search(r'\\[INST\\](.*?)\\[\\/INST\\]', text, re.DOTALL)\n",
        "                sop_match = re.search(r'\\[SOP\\](.*?)\\[\\/SOP\\]', text, re.DOTALL)\n",
        "\n",
        "                if instruction_match and sop_match:\n",
        "                    prompts.append(instruction_match.group(1).strip())\n",
        "                    references.append(sop_match.group(1).strip())\n",
        "\n",
        "        # Sample the prompts\n",
        "        if prompts:\n",
        "            sample_indices = random.sample(range(len(prompts)), min(num_samples, len(prompts)))\n",
        "            return [prompts[i] for i in sample_indices], [references[i] for i in sample_indices]\n",
        "\n",
        "        return [], []\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Evaluate fine-tuned SOP model\")\n",
        "    parser.add_argument(\"--model_path\", required=True, help=\"Path to the fine-tuned model\")\n",
        "    parser.add_argument(\"--dataset\", required=True, help=\"Path to the dataset file used for training\")\n",
        "    parser.add_argument(\"--output\", default=\"evaluation_results.json\", help=\"Path to save evaluation results\")\n",
        "    parser.add_argument(\"--num_samples\", type=int, default=10, help=\"Number of samples to evaluate\")\n",
        "    parser.add_argument(\"--held_out\", action=\"store_true\", help=\"Use completely new prompts not in the dataset\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Sample test prompts\n",
        "    if args.held_out:\n",
        "        test_prompts = sample_test_prompts(args.dataset, args.num_samples, held_out=True)\n",
        "        reference_sops = None\n",
        "    else:\n",
        "        test_prompts, reference_sops = sample_test_prompts(args.dataset, args.num_samples, held_out=False)\n",
        "\n",
        "    # Evaluate model\n",
        "    results, completeness_score = evaluate_model(args.model_path, test_prompts, reference_sops)\n",
        "\n",
        "    # Add summary metrics\n",
        "    summary = {\n",
        "        \"completeness_score\": completeness_score,\n",
        "    }\n",
        "\n",
        "    # Add ROUGE scores if available\n",
        "    if reference_sops:\n",
        "        rouge1_scores = [r.get(\"rouge1\", 0) for r in results if \"rouge1\" in r]\n",
        "        rouge2_scores = [r.get(\"rouge2\", 0) for r in results if \"rouge2\" in r]\n",
        "        rougeL_scores = [r.get(\"rougeL\", 0) for r in results if \"rougeL\" in r]\n",
        "\n",
        "        if rouge1_scores:\n",
        "            summary[\"avg_rouge1\"] = sum(rouge1_scores) / len(rouge1_scores)\n",
        "            summary[\"avg_rouge2\"] = sum(rouge2_scores) / len(rouge2_scores)\n",
        "            summary[\"avg_rougeL\"] = sum(rougeL_scores) / len(rougeL_scores)\n",
        "\n",
        "    # Save results\n",
        "    with open(args.output, 'w', encoding='utf-8') as f:\n",
        "        json.dump({\"results\": results, \"summary\": summary}, f, indent=2)\n",
        "\n",
        "    print(f\"Evaluation complete! Results saved to {args.output}\")\n",
        "    print(f\"Overall completeness score: {completeness_score:.2f}\")\n",
        "\n",
        "    if \"avg_rouge1\" in summary:\n",
        "        print(f\"Average ROUGE-1: {summary['avg_rouge1']:.4f}\")\n",
        "        print(f\"Average ROUGE-2: {summary['avg_rouge2']:.4f}\")\n",
        "        print(f\"Average ROUGE-L: {summary['avg_rougeL']:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    if \"ipykernel\" in sys.modules:\n",
        "        # Simulate command-line args in notebook\n",
        "        class Args:\n",
        "            model_path = \"/content/drive/MyDrive/mistral_sop_finetuned\"\n",
        "            dataset = \"/content/drive/MyDrive/manasa_cleaned_file.jsonl\"\n",
        "            output = \"evaluation_results.json\"\n",
        "            num_samples = 10\n",
        "            held_out = False\n",
        "        args = Args()\n",
        "\n",
        "        # Run the functions directly\n",
        "        if args.held_out:\n",
        "            test_prompts = sample_test_prompts(args.dataset, args.num_samples, held_out=True)\n",
        "            reference_sops = None\n",
        "        else:\n",
        "            test_prompts, reference_sops = sample_test_prompts(args.dataset, args.num_samples, held_out=False)\n",
        "\n",
        "        results, completeness_score = evaluate_model(args.model_path, test_prompts, reference_sops)\n",
        "\n",
        "        with open(args.output, 'w', encoding='utf-8') as f:\n",
        "            json.dump({\"results\": results, \"summary\": {\n",
        "                \"completeness_score\": completeness_score\n",
        "            }}, f, indent=2)\n",
        "\n",
        "        print(f\"Evaluation complete! Results saved to {args.output}\")\n",
        "    else:\n",
        "        main()\n"
      ],
      "metadata": {
        "id": "VOgvk670F-Dz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364,
          "referenced_widgets": [
            "590318846eb4486a966bafaef493a366",
            "1fe82fb3a739474d94f9bd6d58585735",
            "058562aa15bd470a8ebede26e47212f8",
            "a577a70be12e4e6a866fcac2c77ca2ee",
            "2e2cefff4bbd419c87421fe891602198",
            "f3862f505944423dbf213b4168925b79",
            "0248d86b8cf14b64a080829f8e29d183",
            "f597c15e4dc34b729606d7d404e2fa49",
            "f76e0a8750d04a71ba9c613fce3d4018",
            "b080442691a84a6db07e9ea1f4aae8b5",
            "3adf3bcc0e3a4db59db221c605227767"
          ]
        },
        "outputId": "6ad33ba5-2d46-4156-dfc9-2f50990a7fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "590318846eb4486a966bafaef493a366"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating SOPs:   0%|          | 0/10 [00:00<?, ?it/s]Device set to use cuda:0\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n",
            "Generating SOPs:  10%|█         | 1/10 [03:33<31:57, 213.04s/it]Device set to use cuda:0\n",
            "Generating SOPs:  20%|██        | 2/10 [07:05<28:21, 212.73s/it]Device set to use cuda:0\n",
            "Generating SOPs:  30%|███       | 3/10 [10:39<24:53, 213.31s/it]Device set to use cuda:0\n",
            "Generating SOPs:  40%|████      | 4/10 [14:12<21:19, 213.22s/it]Device set to use cuda:0\n",
            "Generating SOPs:  50%|█████     | 5/10 [17:44<17:44, 212.85s/it]Device set to use cuda:0\n",
            "Generating SOPs:  60%|██████    | 6/10 [21:17<14:10, 212.71s/it]Device set to use cuda:0\n",
            "Generating SOPs:  70%|███████   | 7/10 [24:49<10:37, 212.65s/it]Device set to use cuda:0\n",
            "Generating SOPs:  80%|████████  | 8/10 [28:21<07:04, 212.43s/it]Device set to use cuda:0\n",
            "Generating SOPs:  90%|█████████ | 9/10 [31:54<03:32, 212.39s/it]Device set to use cuda:0\n",
            "Generating SOPs: 100%|██████████| 10/10 [35:26<00:00, 212.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation complete! Results saved to evaluation_results.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import json\n",
        "\n",
        "# Define paths\n",
        "jsonl_path = '/content/drive/MyDrive/manasa_cleaned_file.jsonl'  # Update this to your JSONL file path\n",
        "\n",
        "# Load dataset from JSONL\n",
        "def load_jsonl_data(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "sop_data = load_jsonl_data(jsonl_path)\n",
        "\n",
        "\n",
        "# Create HF Dataset\n",
        "dataset = Dataset.from_list(sop_data)\n",
        "\n",
        "# Split the dataset into training and validation\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = dataset[\"train\"]\n",
        "eval_dataset = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "Fb3DhSDHsaf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets peft bert-score rouge-score spacy textstat nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIllDDQYtbJ0",
        "outputId": "575ca284-15a9-4ee2-a56f-7609acbada2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/61.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Pipeline for Fine-tuned Mistral-7B-Instruct SOP Generator\n",
        "# This code assumes you have already fine-tuned your model and saved it\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from rouge_score import rouge_scorer\n",
        "from datasets import Dataset\n",
        "from bert_score import score as bert_score\n",
        "import spacy\n",
        "import textstat\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q transformers datasets peft bert-score rouge-score spacy textstat nltk\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load spaCy model\n",
        "!python -m spacy download en_core_web_md\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx7mhAxNu_DH",
        "outputId": "5cc45d4b-c53e-4df8-ce1b-17b2380836af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "# Configure paths\n",
        "model_path = '/content/drive/MyDrive/mistral_sop_finetuned'  # Path to your fine-tuned model\n",
        "eval_output_dir = '/content/drive/MyDrive/sop_evaluation_results'  # Where to save evaluation results\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(eval_output_dir, exist_ok=True)\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "print(\"Loading model and tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "\n",
        "# First check if this is a PEFT/LoRA model\n",
        "is_peft_model = False\n",
        "try:\n",
        "    peft_config = PeftConfig.from_pretrained(model_path)\n",
        "    is_peft_model = True\n",
        "    print(\"Detected PEFT/LoRA model, loading base model first...\")\n",
        "\n",
        "    # Load the base model with proper quantization\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        peft_config.base_model_name_or_path,\n",
        "        load_in_4bit=True,  # Use 4-bit precision\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Load the LoRA adapter on top of it\n",
        "    model = PeftModel.from_pretrained(base_model, model_path)\n",
        "    print(\"Successfully loaded PEFT/LoRA model\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Not a PEFT model or error loading PEFT config: {e}\")\n",
        "    print(\"Loading as standard model...\")\n",
        "\n",
        "    # Load as standard model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "# Define initial test prompts - include both seen and unseen domains\n",
        "test_prompts = [\n",
        "    \"Write me an SOP for pursuing a Master's Degree in Computer Science focusing on Artificial Intelligence.\",  # Similar to training\n",
        "    \"Write me an SOP for pursuing a Master's Degree in Data Science at MIT.\",  # New domain but similar degree\n",
        "    \"Write me an SOP for pursuing a PhD in Biology with focus on Genetics.\",  # Different degree and field\n",
        "    \"Write me an SOP for pursuing an MBA with concentration in Finance.\",  # Very different domain\n",
        "    \"Write me an SOP for a Master's in Fine Arts focusing on Digital Media.\",  # Creative field\n",
        "]\n",
        "\n",
        "# Get reference SOPs for style comparison from your eval_dataset\n",
        "reference_sops = []\n",
        "for example in eval_dataset:\n",
        "    # Extract the instruction and output from your dataset\n",
        "    # This assumes your eval_dataset has the same format as what you used for training\n",
        "    text = example[\"text\"]\n",
        "\n",
        "    # Parse the instruction and output from the formatted text\n",
        "    # Assuming format is \"<s>[INST] instruction [/INST] output </s>\"\n",
        "    try:\n",
        "        instruction = text.split(\"[INST]\")[1].split(\"[/INST]\")[0].strip()\n",
        "        output = text.split(\"[/INST]\")[1].split(\"</s>\")[0].strip()\n",
        "\n",
        "        reference_sops.append({\n",
        "            \"instruction\": instruction,\n",
        "            \"output\": output\n",
        "        })\n",
        "\n",
        "        # Add this instruction to test prompts if not already there\n",
        "        if instruction not in test_prompts:\n",
        "            test_prompts.append(instruction)\n",
        "\n",
        "    except IndexError:\n",
        "        print(f\"Could not parse example: {text[:50]}...\")\n",
        "\n",
        "# Use these reference SOPs for evaluation\n",
        "reference_texts = [item[\"output\"] for item in reference_sops if \"output\" in item]\n",
        "\n",
        "print(f\"Loaded {len(reference_texts)} reference SOPs from evaluation dataset\")\n",
        "print(f\"Total test prompts for evaluation: {len(test_prompts)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397,
          "referenced_widgets": [
            "32c46e3828434632b8deacc01f436648",
            "7a44d5374ebe49018e204215b35d46fb",
            "6159c078713742c0bc6d8ae68e017552",
            "a343a41c20034de68ca2808b9400a016",
            "12701f041ec741958df18bd25f79e30a",
            "3fadd2e1656c44c291645761bdffc6d2",
            "0df50454c9604bc5b509e10de5638f3a",
            "cafc1470ecad41269f81325ffe776e5d",
            "9b9367adf5b2464b8b2c7ff157263ce4",
            "74346ee891a541da967147965a985aba",
            "1d46ebbbcd9b443585640b73b6bcc6eb",
            "0f75bf81e9b846519d737c2f537e6e3b",
            "3805760f38e646ffac61971c0e89a958",
            "c20a6d0f1103421f9fff0d237efb1aa4",
            "82fa57f141e84d52a0be8bdbf6b40a22",
            "848afb66280e42c4a618c9da31e03618",
            "4c48ab7ff00e4a92a0e7b8aaacd093b8",
            "aef459335b424085928ea608b7685c07",
            "65a3128a9c584e89a73b437ded813936",
            "8cbdf6f9e09a46d4a37c1d7ff5aa5bcb",
            "25ab9376cbd5489b87b8f15a149a03ef",
            "ebfc13aa097b41b9b80f2c2d83bc30bd",
            "aa17f75330214ef7b7aef03f8b6160e1",
            "6257c0adfe1942adb54c85027e6af6fe",
            "17122505fa7249b09e2ea2de19b7fd19",
            "3154fcb147cf4a79b86e4a6ab06a9270",
            "fe735a29e4db45aeaba48427e8c6f12e",
            "a011c0f257f64b6688a8f26dd838b6c8",
            "0e2e4ca84dc744e285e86c7851232c86",
            "aeed61c8cd26440b881f86ab93ba420b",
            "fe88eee57c9646d4b345657809b2f1ca",
            "59b1e44429e44cbf969ddba0a498201a",
            "00e441735d9f4ad19274d4f94593d549",
            "1e5e8d860b7447b6a35f7e691b9f644a",
            "ac70a5d9d6f34a7bbb4767a56ac407ba",
            "c68a4e4e91e34c0fab031035fef14063",
            "49fd2197a3224be2aae5da4c0ab883ea",
            "1b76514142a145bda5e9a59a296b5f04",
            "b3dce121f13649c4ae99fe8b4f84a45c",
            "b2086b51bbb64df5806662215c24cd81",
            "60a0001935db43828b7713e2382de6af",
            "08516d766a1949f49b41809c5a587c28",
            "6b5f745d341f4708b2b13ae90f1d886b",
            "533ed34147854cdca8316e7c3755aa48",
            "5f234d443dda4c7cb415cc9f5f8bc295",
            "2fcd5c0743d9423b9b62ddd8d6ed9709",
            "3fac4a25b11b4a959e66f643ed110a0b",
            "f87ce0248c0f42cb81c9eb9cfdadf794",
            "ea3d0ebefd2d47b8bf9baede6f70c945",
            "774c3e2d9dac4d2b8f73cb0298aa34c4",
            "cb8d88ef1ea1469a85a2b4871cee7825",
            "c380c2b1e1e54f93a6778b5ed4f65912",
            "af5867865649400ea97c3c809b988d90",
            "bffb06e55ccb4126abeefad137ebf296",
            "247d83557bf54318ae060f5cd0465ab3",
            "8bdb969c8c0949dfbcc2b763a5ebfe50",
            "fd2dbc8a270a42dba5ace810c017af36",
            "5d47924e8e6d463d8a2405bcf6a33841",
            "43c4feedf44c46f1a03cc9fe44abfe86",
            "d5de5ede62024c448d49cd5e02eaed67",
            "530ce5aba79a4573a3908ed470ab4802",
            "70cb9a12fbf540c8a5312cae8f959cdb",
            "f6a92eb29cea4766ad2310f7fb2852f4",
            "794cbbb956694af5b5b389aa7115c756",
            "59eaaca5beef4c52a1125b462415965c",
            "209cb4c5e6a94d8098da68fae5293a4f",
            "ea34c89fcdfb4855a5e15de54195d2e4",
            "54485f21648744aeb83a17bb7868eb3e",
            "ea251c36c9994e82a33dcf7b03d546c2",
            "97d8f8c5640349eab3e84286d5fe9015",
            "3d6530efbe1e44c5bc2f64040dae79d7",
            "18d7c0dd6e6f44b58e97763954f2d064",
            "07a03728272c400fb76140bb8c1c6004",
            "b233e9d88a3443b0b034790405e381d3",
            "9136ae52fb8c4cbe9e1a3aa3656649e6",
            "644055f2b0e54c0e88c6094c8b3801e0",
            "75ae8044b8bf4a67b956c842594f01cd",
            "8701403657ab4f2aa8f649f89307686d",
            "f0a341258e284bf0be8fa0f1fec951d7",
            "f37b246b0f3a4be1bd0c01110f28b3f4",
            "353b991b28f74de8b729f9bb432dcbc5",
            "3a8429d34b254ebaa6be619bde843723",
            "c35fb76b83724d3eba35e242668c4a42",
            "6f30f3f674964ae6867039eeed8a2a7b",
            "2ae2600da0f846c286efe184bc650df2",
            "c7c7a82632344b948a95fba999eee7db",
            "48a1e50d08ed4c09adb5cd9e970353ed",
            "4b8fc7b86344409e9b42b42fc915a25c"
          ]
        },
        "id": "hO-AgAVyofpG",
        "outputId": "0e43e0c6-4a96-4ff4-92f5-500077201b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model and tokenizer...\n",
            "Detected PEFT/LoRA model, loading base model first...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32c46e3828434632b8deacc01f436648"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f75bf81e9b846519d737c2f537e6e3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa17f75330214ef7b7aef03f8b6160e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e5e8d860b7447b6a35f7e691b9f644a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f234d443dda4c7cb415cc9f5f8bc295"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bdb969c8c0949dfbcc2b763a5ebfe50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea34c89fcdfb4855a5e15de54195d2e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8701403657ab4f2aa8f649f89307686d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded PEFT/LoRA model\n",
            "Loaded 56 reference SOPs from evaluation dataset\n",
            "Total test prompts for evaluation: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate SOPs using the model - optimized version\n",
        "def generate_sop(prompt, max_length=2048):\n",
        "    input_text = f\"<s>[INST] {prompt} [/INST]\\n\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate text with slightly reduced parameters\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=0.7,\n",
        "            top_p=0.85,\n",
        "            do_sample=True,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # More efficient text processing\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    try:\n",
        "        response = generated_text.split(\"[/INST]\")[1].strip()\n",
        "        if \"</s>\" in response:\n",
        "            response = response.split(\"</s>\")[0].strip()\n",
        "    except IndexError:\n",
        "        response = generated_text\n",
        "\n",
        "    return response\n",
        "\n",
        "# Optimized Evaluation functions\n",
        "class SOPEvaluator:\n",
        "    def __init__(self, reference_texts):\n",
        "        self.reference_texts = reference_texts\n",
        "\n",
        "        # Only initialize TF-IDF if we have reference texts\n",
        "        if reference_texts and len(reference_texts) > 0:\n",
        "            self.tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Reduced from 10000\n",
        "            self.tfidf_vectorizer.fit(reference_texts)\n",
        "            self.reference_vectors = self.tfidf_vectorizer.transform(reference_texts)\n",
        "        else:\n",
        "            self.tfidf_vectorizer = None\n",
        "            self.reference_vectors = None\n",
        "\n",
        "        # Initialize ROUGE scorer with only essential metrics\n",
        "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)  # Removed rouge2\n",
        "\n",
        "        # Initialize trackers\n",
        "        self.all_metrics = []\n",
        "\n",
        "        # Pre-compile regex patterns\n",
        "        self.sentence_end_pattern = re.compile(r'[.!?]$')\n",
        "\n",
        "    def check_completeness(self, text):\n",
        "        \"\"\"Check if the SOP is complete (not ending mid-sentence)\"\"\"\n",
        "        if text and len(text.strip()) > 0:\n",
        "            return bool(self.sentence_end_pattern.search(text.strip()))\n",
        "        return False\n",
        "\n",
        "    def measure_style_similarity(self, text):\n",
        "        \"\"\"Measure similarity to reference style using TF-IDF vectors\"\"\"\n",
        "        if not self.reference_vectors is None:\n",
        "            try:\n",
        "                # Transform the new text to TF-IDF\n",
        "                text_vector = self.tfidf_vectorizer.transform([text])\n",
        "\n",
        "                # Calculate cosine similarities with each reference\n",
        "                similarities = cosine_similarity(text_vector, self.reference_vectors)[0]\n",
        "\n",
        "                # Return the highest similarity score\n",
        "                return np.max(similarities) if similarities.size > 0 else 0\n",
        "            except:\n",
        "                return 0\n",
        "        return 0\n",
        "\n",
        "    def calculate_readability(self, text):\n",
        "        \"\"\"Calculate readability metrics - optimized to do fewer calculations\"\"\"\n",
        "        flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
        "        # Only calculate grade level if needed\n",
        "        flesch_kincaid_grade = textstat.flesch_kincaid_grade(text)\n",
        "\n",
        "        return {\n",
        "            \"flesch_reading_ease\": flesch_reading_ease,\n",
        "            \"flesch_kincaid_grade\": flesch_kincaid_grade\n",
        "        }\n",
        "\n",
        "    def analyze_structure(self, text):\n",
        "        \"\"\"Analyze the structure of the SOP - optimized\"\"\"\n",
        "        # Use faster sentence tokenization\n",
        "        sentences = text.split('. ')\n",
        "        paragraphs = text.split('\\n\\n')\n",
        "\n",
        "        # Check SOP pattern with simple string check\n",
        "        has_sop_header = text.strip().startswith(\"STATEMENT OF PURPOSE\")\n",
        "\n",
        "        # Calculate average sentence length more efficiently\n",
        "        if sentences:\n",
        "            total_words = sum(len(s.split()) for s in sentences)\n",
        "            avg_sent_len = total_words / len(sentences)\n",
        "        else:\n",
        "            avg_sent_len = 0\n",
        "\n",
        "        return {\n",
        "            \"num_sentences\": len(sentences),\n",
        "            \"num_paragraphs\": len(paragraphs),\n",
        "            \"avg_sentence_length\": avg_sent_len,\n",
        "            \"has_sop_header\": has_sop_header\n",
        "        }\n",
        "\n",
        "    def calculate_semantic_richness(self, text):\n",
        "        \"\"\"Calculate semantic richness metrics - simplified\"\"\"\n",
        "        # Use a simpler approach to estimate vocabulary richness\n",
        "        words = text.lower().split()\n",
        "        unique_words = set(words)\n",
        "\n",
        "        # Skip NLP processing for entities to speed up\n",
        "        return {\n",
        "            \"vocabulary_richness\": len(unique_words) / len(words) if words else 0,\n",
        "            \"num_entities\": 0  # Skip entity extraction to save time\n",
        "        }\n",
        "\n",
        "    def evaluate_sop(self, generated_text, prompt):\n",
        "        \"\"\"Comprehensive evaluation of a generated SOP - optimized\"\"\"\n",
        "        # Basic metrics\n",
        "        length = len(generated_text.split())\n",
        "        is_complete = self.check_completeness(generated_text)\n",
        "\n",
        "        # Only calculate style similarity if we have reference texts\n",
        "        style_similarity = self.measure_style_similarity(generated_text) if self.reference_texts else 0\n",
        "\n",
        "        # ROUGE scores - limit to first reference only\n",
        "        rouge_scores = {}\n",
        "        if self.reference_texts:\n",
        "            scores = self.rouge_scorer.score(generated_text, self.reference_texts[0])\n",
        "            rouge_scores = {f\"rouge_{metric}\": score.fmeasure for metric, score in scores.items()}\n",
        "\n",
        "        # Readability metrics - only calculate if text is long enough\n",
        "        readability = self.calculate_readability(generated_text) if length > 50 else {\"flesch_reading_ease\": 0, \"flesch_kincaid_grade\": 0}\n",
        "\n",
        "        # Structure analysis\n",
        "        structure = self.analyze_structure(generated_text)\n",
        "\n",
        "        # Semantic richness - simplified\n",
        "        semantics = self.calculate_semantic_richness(generated_text)\n",
        "\n",
        "        # Combine all metrics\n",
        "        metrics = {\n",
        "            \"prompt\": prompt[:50],  # Store only first 50 chars of prompt to save memory\n",
        "            \"length\": length,\n",
        "            \"is_complete\": is_complete,\n",
        "            \"style_similarity\": style_similarity,\n",
        "            **rouge_scores,\n",
        "            **readability,\n",
        "            **structure,\n",
        "            **semantics\n",
        "        }\n",
        "\n",
        "        self.all_metrics.append(metrics)\n",
        "        return metrics\n",
        "\n",
        "    def generate_report(self, output_dir):\n",
        "        \"\"\"Generate a comprehensive evaluation report - optimized\"\"\"\n",
        "        # Convert metrics to DataFrame\n",
        "        df = pd.DataFrame(self.all_metrics)\n",
        "\n",
        "        # Save detailed metrics to CSV\n",
        "        csv_path = os.path.join(output_dir, \"evaluation_metrics.csv\")\n",
        "        df.to_csv(csv_path, index=False)\n",
        "\n",
        "        # Calculate summary statistics\n",
        "        summary = {\n",
        "            \"avg_length\": df[\"length\"].mean(),\n",
        "            \"completeness_rate\": df[\"is_complete\"].mean() * 100,\n",
        "            \"avg_style_similarity\": df[\"style_similarity\"].mean() if \"style_similarity\" in df else 0,\n",
        "            \"avg_flesch_reading_ease\": df[\"flesch_reading_ease\"].mean(),\n",
        "            \"avg_flesch_kincaid_grade\": df[\"flesch_kincaid_grade\"].mean(),\n",
        "            \"avg_num_paragraphs\": df[\"num_paragraphs\"].mean(),\n",
        "            \"avg_sentence_length\": df[\"avg_sentence_length\"].mean(),\n",
        "            \"sop_header_rate\": df[\"has_sop_header\"].mean() * 100,\n",
        "            \"avg_vocabulary_richness\": df[\"vocabulary_richness\"].mean()\n",
        "        }\n",
        "\n",
        "        # Save summary to JSON\n",
        "        summary_path = os.path.join(output_dir, \"evaluation_summary.json\")\n",
        "        with open(summary_path, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "        # Generate only essential visualizations\n",
        "        self._generate_essential_visualizations(df, output_dir)\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def _generate_essential_visualizations(self, df, output_dir):\n",
        "        \"\"\"Generate only the most important visualizations\"\"\"\n",
        "        # Style similarity across prompts - if we have this data\n",
        "        if \"style_similarity\" in df.columns:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.bar(range(len(df)), df[\"style_similarity\"], color='skyblue')\n",
        "            plt.xlabel(\"Test Case\")\n",
        "            plt.ylabel(\"Style Similarity Score\")\n",
        "            plt.title(\"Style Similarity to Reference SOPs\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(output_dir, \"style_similarity.png\"))\n",
        "            plt.close()\n",
        "\n",
        "        # Length distribution - essential\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.hist(df[\"length\"], bins=10, color='green', alpha=0.7)  # Reduced bins from 15 to 10\n",
        "        plt.xlabel(\"SOP Length (words)\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.title(\"Distribution of SOP Lengths\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, \"length_distribution.png\"))\n",
        "        plt.close()\n",
        "\n",
        "# Main evaluation function - optimized\n",
        "def evaluate_model(test_prompts, evaluator, save_generations=True, sample_size=None):\n",
        "    \"\"\"Run evaluation on the model with option to sample prompts\"\"\"\n",
        "    # Optionally sample a subset of prompts for faster evaluation\n",
        "    if sample_size and sample_size < len(test_prompts):\n",
        "        test_prompts = random.sample(test_prompts, sample_size)\n",
        "\n",
        "    print(f\"Evaluating model on {len(test_prompts)} test prompts...\")\n",
        "\n",
        "    generated_sops = {}\n",
        "\n",
        "    # Generate SOPs for each test prompt\n",
        "    for prompt in tqdm(test_prompts):\n",
        "        generated_text = generate_sop(prompt)\n",
        "        generated_sops[prompt] = generated_text\n",
        "\n",
        "        # Evaluate the generated SOP\n",
        "        metrics = evaluator.evaluate_sop(generated_text, prompt)\n",
        "\n",
        "        # Print minimal feedback\n",
        "        print(f\"Processed prompt: {prompt[:30]}... ({metrics['length']} words)\")\n",
        "\n",
        "    # Save all generated SOPs\n",
        "    if save_generations:\n",
        "        sops_path = os.path.join(eval_output_dir, \"generated_sops.json\")\n",
        "        with open(sops_path, 'w') as f:\n",
        "            json.dump(generated_sops, f, indent=2)\n",
        "\n",
        "    # Generate evaluation report\n",
        "    summary = evaluator.generate_report(eval_output_dir)\n",
        "\n",
        "    print(\"\\n==== Evaluation Summary ====\")\n",
        "    for metric, value in summary.items():\n",
        "        print(f\"{metric}: {value:.2f}\")\n",
        "\n",
        "    return summary, generated_sops\n",
        "\n",
        "# Optimized BERTScore evaluation\n",
        "def calculate_bert_scores(generated_sops, reference_texts, sample_size=None):\n",
        "    \"\"\"Calculate BERTScore with option to sample for faster evaluation\"\"\"\n",
        "    if not reference_texts or not generated_sops:\n",
        "        print(\"Missing either reference or generated texts for BERTScore evaluation\")\n",
        "        return {}\n",
        "\n",
        "    # Sample prompts if requested\n",
        "    prompts = list(generated_sops.keys())\n",
        "    if sample_size and sample_size < len(prompts):\n",
        "        sampled_prompts = random.sample(prompts, sample_size)\n",
        "        sampled_sops = {p: generated_sops[p] for p in sampled_prompts}\n",
        "    else:\n",
        "        sampled_sops = generated_sops\n",
        "\n",
        "    generated_texts = list(sampled_sops.values())\n",
        "\n",
        "    # Use first reference text\n",
        "    reference_text = reference_texts[0] if reference_texts else \"\"\n",
        "    references = [reference_text] * len(generated_texts)\n",
        "\n",
        "    # Calculate BERTScore with lower batch size\n",
        "    try:\n",
        "        print(\"Calculating BERTScore (this may take a while)...\")\n",
        "        P, R, F1 = bert_score(generated_texts, references, lang=\"en\", batch_size=8, verbose=True)\n",
        "\n",
        "        bert_scores = {\n",
        "            \"bert_precision\": P.mean().item(),\n",
        "            \"bert_recall\": R.mean().item(),\n",
        "            \"bert_f1\": F1.mean().item()\n",
        "        }\n",
        "\n",
        "        # Save only summary scores to save time\n",
        "        with open(os.path.join(eval_output_dir, \"bertscore_summary.json\"), 'w') as f:\n",
        "            json.dump(bert_scores, f, indent=2)\n",
        "\n",
        "        return bert_scores\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating BERTScore: {e}\")\n",
        "        return {}\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Add these imports at the top of your file if not already there\n",
        "    import random\n",
        "    import re\n",
        "\n",
        "    # Define sample size for faster evaluation\n",
        "    SAMPLE_SIZE = 20  # Change this number to control evaluation speed\n",
        "\n",
        "    # Initialize evaluator\n",
        "    evaluator = SOPEvaluator(reference_texts)\n",
        "\n",
        "    # Run evaluation with sampling\n",
        "    summary, generated_sops = evaluate_model(test_prompts, evaluator, sample_size=SAMPLE_SIZE)\n",
        "\n",
        "    # Calculate BERTScore on a small sample for speed\n",
        "    bert_scores = calculate_bert_scores(generated_sops, reference_texts, sample_size=min(10, len(generated_sops)))\n",
        "\n",
        "    if bert_scores:\n",
        "        print(\"\\n==== BERTScore Results ====\")\n",
        "        for metric, value in bert_scores.items():\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "        # Update summary with BERTScore\n",
        "        with open(os.path.join(eval_output_dir, \"evaluation_summary.json\"), 'r') as f:\n",
        "            full_summary = json.load(f)\n",
        "\n",
        "        full_summary.update(bert_scores)\n",
        "\n",
        "        with open(os.path.join(eval_output_dir, \"evaluation_summary.json\"), 'w') as f:\n",
        "            json.dump(full_summary, f, indent=2)\n",
        "\n",
        "    # Generate a simplified report\n",
        "    with open(os.path.join(eval_output_dir, \"evaluation_report.md\"), 'w') as f:\n",
        "        f.write(\"# SOP Generation Model Evaluation Report\\n\\n\")\n",
        "        f.write(f\"Evaluation performed on {len(generated_sops)} test prompts\\n\\n\")\n",
        "\n",
        "        f.write(\"## Overall Metrics\\n\\n\")\n",
        "        f.write(f\"- Average SOP Length: {summary['avg_length']:.1f} words\\n\")\n",
        "        f.write(f\"- Completeness Rate: {summary['completeness_rate']:.1f}%\\n\")\n",
        "        f.write(f\"- Style Similarity to Reference: {summary['avg_style_similarity']:.3f}\\n\")\n",
        "        f.write(f\"- SOP Header Rate: {summary['sop_header_rate']:.1f}%\\n\\n\")\n",
        "\n",
        "        f.write(\"## Readability & Structure\\n\\n\")\n",
        "        f.write(f\"- Flesch Reading Ease: {summary['avg_flesch_reading_ease']:.1f}\\n\")\n",
        "        f.write(f\"- Flesch-Kincaid Grade Level: {summary['avg_flesch_kincaid_grade']:.1f}\\n\")\n",
        "        f.write(f\"- Average Paragraphs: {summary['avg_num_paragraphs']:.1f}\\n\")\n",
        "        f.write(f\"- Average Sentence Length: {summary['avg_sentence_length']:.1f} words\\n\\n\")\n",
        "\n",
        "        if bert_scores:\n",
        "            f.write(\"## Semantic Similarity (BERTScore)\\n\\n\")\n",
        "            f.write(f\"- F1 Score: {bert_scores['bert_f1']:.4f}\\n\\n\")\n",
        "\n",
        "        # Show only one example to save space\n",
        "        f.write(\"## Example Generation\\n\\n\")\n",
        "        first_prompt = next(iter(generated_sops.keys()))\n",
        "        f.write(f\"**Prompt:** {first_prompt[:100]}...\\n\\n\")\n",
        "        f.write(\"**Generated SOP:**\\n\\n\")\n",
        "        f.write(f\"```\\n{generated_sops[first_prompt][:300]}...\\n```\\n\\n\")\n",
        "\n",
        "    print(f\"\\nEvaluation complete. Results saved to {eval_output_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "Bnorz5QavNR3",
        "outputId": "f492a869-bccf-4a3d-9576-dc0d51d545d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model on 20 test prompts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n",
            "  5%|▌         | 1/20 [03:41<1:10:02, 221.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed prompt: Write me an SOP for pursuing a... (1764 words)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 2/20 [07:20<1:06:00, 220.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed prompt: Write me an SOP for pursuing a... (1795 words)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 3/20 [10:59<1:02:13, 219.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed prompt: Write me an SOP for pursuing a... (1685 words)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 3/20 [13:16<1:15:16, 265.65s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-80e880a2cc77>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;31m# Run evaluation with sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m     \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_sops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSAMPLE_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;31m# Calculate BERTScore on a small sample for speed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-80e880a2cc77>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(test_prompts, evaluator, save_generations, sample_size)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;31m# Generate SOPs for each test prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_prompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_sop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0mgenerated_sops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-80e880a2cc77>\u001b[0m in \u001b[0;36mgenerate_sop\u001b[0;34m(prompt, max_length)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Generate text with slightly reduced parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         outputs = model.generate(\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2465\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2466\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3432\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3434\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3436\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    811\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m                 )\n\u001b[1;32m    535\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    537\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m                 \u001b[0;31m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;31m# The reason is that in some cases, an error can occur that backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul4Bit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemv_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\u001b[0m in \u001b[0;36mgemv_4bit\u001b[0;34m(A, B, out, transposed_A, transposed_B, state)\u001b[0m\n\u001b[1;32m   2032\u001b[0m                 )\n\u001b[1;32m   2033\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2034\u001b[0;31m                 lib.cgemm_4bit_inference_naive_fp32(\n\u001b[0m\u001b[1;32m   2035\u001b[0m                     \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m                     \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SOP Generation Model Evaluation Pipeline\n",
        "# Optimized for Google Colab with A100 GPU\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "import textstat\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from bert_score import BERTScorer\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q rouge_score textstat transformers bert_score tqdm nltk\n",
        "!nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# Set up environment\n",
        "print(\"Setting up environment...\")\n",
        "\n",
        "# Create output directory\n",
        "eval_output_dir = \"sop_evaluation_results\"\n",
        "os.makedirs(eval_output_dir, exist_ok=True)\n",
        "\n",
        "# Enable GPU acceleration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Model setup\n",
        "def load_model(model_name_or_path):\n",
        "    \"\"\"Load model and tokenizer with optimized settings for A100\"\"\"\n",
        "    print(f\"Loading model: {model_name_or_path}\")\n",
        "\n",
        "    # Configure model loading for optimal A100 performance\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        torch_dtype=torch.bfloat16,  # Use bfloat16 for better A100 performance\n",
        "        device_map=\"auto\",           # Optimize device mapping\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        use_fast=True,               # Use fast tokenizer\n",
        "        padding_side=\"left\",         # For causal models\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    # Add padding token if needed\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# SOP Generation Function\n",
        "def generate_sop(prompt, model, tokenizer, max_length=2048):\n",
        "    \"\"\"Generate SOP using the model with optimized settings for A100\"\"\"\n",
        "    input_text = f\"<s>[INST] {prompt} [/INST]\\n\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate text with performance-optimized settings\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=0.7,\n",
        "            top_p=0.85,\n",
        "            do_sample=True,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Process generated text\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    try:\n",
        "        response = generated_text.split(\"[/INST]\")[1].strip()\n",
        "        if \"</s>\" in response:\n",
        "            response = response.split(\"</s>\")[0].strip()\n",
        "    except IndexError:\n",
        "        response = generated_text\n",
        "\n",
        "    return response\n",
        "\n",
        "# Optimized Batch Generation\n",
        "def batch_generate_sops(prompts, model, tokenizer, batch_size=4):\n",
        "    \"\"\"Generate SOPs in batches for better GPU utilization\"\"\"\n",
        "    generated_sops = {}\n",
        "\n",
        "    # Process in batches\n",
        "    for i in range(0, len(prompts), batch_size):\n",
        "        batch_prompts = prompts[i:i+batch_size]\n",
        "        print(f\"Processing batch {i//batch_size + 1}/{(len(prompts)-1)//batch_size + 1}\")\n",
        "\n",
        "        # Process each prompt in the batch\n",
        "        for prompt in tqdm(batch_prompts):\n",
        "            generated_text = generate_sop(prompt, model, tokenizer)\n",
        "            generated_sops[prompt] = generated_text\n",
        "\n",
        "    return generated_sops\n",
        "\n",
        "# Fast SOP Evaluator\n",
        "class FastSOPEvaluator:\n",
        "    def __init__(self, reference_texts=None):\n",
        "        self.reference_texts = reference_texts\n",
        "\n",
        "        # Initialize TFIDF only if we have reference texts\n",
        "        if reference_texts and len(reference_texts) > 0:\n",
        "            print(\"Initializing TF-IDF vectorizer...\")\n",
        "            self.tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "            self.tfidf_vectorizer.fit(reference_texts)\n",
        "            self.reference_vectors = self.tfidf_vectorizer.transform(reference_texts)\n",
        "        else:\n",
        "            self.tfidf_vectorizer = None\n",
        "            self.reference_vectors = None\n",
        "\n",
        "        # Pre-compile regex patterns\n",
        "        self.sentence_end_pattern = re.compile(r'[.!?]$')\n",
        "\n",
        "        # Initialize metrics storage\n",
        "        self.all_metrics = []\n",
        "\n",
        "    def check_completeness(self, text):\n",
        "        \"\"\"Fast check if the SOP is complete\"\"\"\n",
        "        if text and len(text.strip()) > 0:\n",
        "            return bool(self.sentence_end_pattern.search(text.strip()))\n",
        "        return False\n",
        "\n",
        "    def measure_style_similarity(self, text):\n",
        "        \"\"\"Fast style similarity calculation\"\"\"\n",
        "        if self.reference_vectors is not None:\n",
        "            try:\n",
        "                text_vector = self.tfidf_vectorizer.transform([text])\n",
        "                similarities = cosine_similarity(text_vector, self.reference_vectors)[0]\n",
        "                return np.max(similarities) if similarities.size > 0 else 0\n",
        "            except:\n",
        "                return 0\n",
        "        return 0\n",
        "\n",
        "    def analyze_structure(self, text):\n",
        "        \"\"\"Fast structure analysis\"\"\"\n",
        "        # Split by common sentence delimiters\n",
        "        sentences = re.split(r'[.!?] ', text)\n",
        "        sentences = [s for s in sentences if s.strip()]\n",
        "\n",
        "        # Count paragraphs\n",
        "        paragraphs = text.split('\\n\\n')\n",
        "        paragraphs = [p for p in paragraphs if p.strip()]\n",
        "\n",
        "        # Check header\n",
        "        has_sop_header = text.strip().startswith(\"STATEMENT OF PURPOSE\")\n",
        "\n",
        "        # Calculate average sentence length\n",
        "        word_count = sum(len(s.split()) for s in sentences)\n",
        "        avg_sent_len = word_count / len(sentences) if sentences else 0\n",
        "\n",
        "        return {\n",
        "            \"num_sentences\": len(sentences),\n",
        "            \"num_paragraphs\": len(paragraphs),\n",
        "            \"avg_sentence_length\": avg_sent_len,\n",
        "            \"has_sop_header\": has_sop_header\n",
        "        }\n",
        "\n",
        "    def calculate_readability(self, text):\n",
        "        \"\"\"Fast readability metrics\"\"\"\n",
        "        return {\n",
        "            \"flesch_reading_ease\": textstat.flesch_reading_ease(text),\n",
        "            \"flesch_kincaid_grade\": textstat.flesch_kincaid_grade(text)\n",
        "        }\n",
        "\n",
        "    def evaluate_sop(self, generated_text, prompt):\n",
        "        \"\"\"Evaluate a single SOP with essential metrics only\"\"\"\n",
        "        # Basic metrics (always calculate)\n",
        "        length = len(generated_text.split())\n",
        "        is_complete = self.check_completeness(generated_text)\n",
        "\n",
        "        # Optional metrics (skip if no references)\n",
        "        style_similarity = self.measure_style_similarity(generated_text) if self.reference_vectors is not None else 0\n",
        "\n",
        "        # Structure analysis\n",
        "        structure = self.analyze_structure(generated_text)\n",
        "\n",
        "        # Readability (only if reasonable length)\n",
        "        if length > 30:\n",
        "            readability = self.calculate_readability(generated_text)\n",
        "        else:\n",
        "            readability = {\"flesch_reading_ease\": 0, \"flesch_kincaid_grade\": 0}\n",
        "\n",
        "        # Vocabulary richness (simple calculation)\n",
        "        words = generated_text.lower().split()\n",
        "        unique_words = set(words)\n",
        "        vocab_richness = len(unique_words) / max(1, len(words))\n",
        "\n",
        "        # Combine metrics\n",
        "        metrics = {\n",
        "            \"prompt\": prompt[:50],  # Truncate prompt for memory efficiency\n",
        "            \"length\": length,\n",
        "            \"is_complete\": is_complete,\n",
        "            \"style_similarity\": style_similarity,\n",
        "            **structure,\n",
        "            **readability,\n",
        "            \"vocabulary_richness\": vocab_richness\n",
        "        }\n",
        "\n",
        "        self.all_metrics.append(metrics)\n",
        "        return metrics\n",
        "\n",
        "    def batch_evaluate(self, generated_sops):\n",
        "        \"\"\"Evaluate all SOPs at once\"\"\"\n",
        "        print(\"Evaluating generated SOPs...\")\n",
        "        results = {}\n",
        "\n",
        "        for prompt, text in tqdm(generated_sops.items()):\n",
        "            metrics = self.evaluate_sop(text, prompt)\n",
        "            results[prompt] = metrics\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_report(self):\n",
        "        \"\"\"Generate evaluation report from collected metrics\"\"\"\n",
        "        # Convert metrics to DataFrame\n",
        "        df = pd.DataFrame(self.all_metrics)\n",
        "\n",
        "        # Save detailed metrics\n",
        "        csv_path = os.path.join(eval_output_dir, \"evaluation_metrics.csv\")\n",
        "        df.to_csv(csv_path, index=False)\n",
        "\n",
        "        # Calculate summary statistics\n",
        "        summary = {\n",
        "            \"avg_length\": df[\"length\"].mean(),\n",
        "            \"completeness_rate\": df[\"is_complete\"].mean() * 100,\n",
        "            \"avg_style_similarity\": df[\"style_similarity\"].mean(),\n",
        "            \"avg_flesch_reading_ease\": df[\"flesch_reading_ease\"].mean(),\n",
        "            \"avg_flesch_kincaid_grade\": df[\"flesch_kincaid_grade\"].mean(),\n",
        "            \"avg_num_paragraphs\": df[\"num_paragraphs\"].mean(),\n",
        "            \"avg_sentence_length\": df[\"avg_sentence_length\"].mean(),\n",
        "            \"sop_header_rate\": df[\"has_sop_header\"].mean() * 100,\n",
        "            \"avg_vocabulary_richness\": df[\"vocabulary_richness\"].mean()\n",
        "        }\n",
        "\n",
        "        # Save summary to JSON\n",
        "        summary_path = os.path.join(eval_output_dir, \"evaluation_summary.json\")\n",
        "        with open(summary_path, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "        # Generate essential visualizations\n",
        "        self.generate_visualizations(df)\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def generate_visualizations(self, df):\n",
        "        \"\"\"Generate essential visualizations only\"\"\"\n",
        "        # Length distribution\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.hist(df[\"length\"], bins=10, color='green', alpha=0.7)\n",
        "        plt.xlabel(\"SOP Length (words)\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.title(\"Distribution of SOP Lengths\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(eval_output_dir, \"length_distribution.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # Style similarity if available\n",
        "        if df[\"style_similarity\"].mean() > 0:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.bar(range(len(df)), df[\"style_similarity\"], color='skyblue')\n",
        "            plt.xlabel(\"Test Case\")\n",
        "            plt.ylabel(\"Style Similarity Score\")\n",
        "            plt.title(\"Style Similarity to Reference SOPs\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(eval_output_dir, \"style_similarity.png\"))\n",
        "            plt.close()\n",
        "\n",
        "# Fast BERTScore evaluation\n",
        "def fast_bert_evaluation(generated_sops, reference_texts=None, sample_size=10):\n",
        "    \"\"\"Calculate BERTScore using optimized settings\"\"\"\n",
        "    if not generated_sops:\n",
        "        print(\"No generated SOPs to evaluate\")\n",
        "        return {}\n",
        "\n",
        "    # Sample a subset for faster evaluation\n",
        "    prompts = list(generated_sops.keys())\n",
        "    if sample_size and sample_size < len(prompts):\n",
        "        sampled_prompts = random.sample(prompts, sample_size)\n",
        "        sampled_sops = {p: generated_sops[p] for p in sampled_prompts}\n",
        "    else:\n",
        "        sampled_sops = generated_sops\n",
        "\n",
        "    generated_texts = list(sampled_sops.values())\n",
        "\n",
        "    # If we have reference texts, use them; otherwise compare to each other\n",
        "    if reference_texts and len(reference_texts) > 0:\n",
        "        reference_text = reference_texts[0]\n",
        "        references = [reference_text] * len(generated_texts)\n",
        "    else:\n",
        "        # Use the first generated text as reference\n",
        "        reference_text = generated_texts[0]\n",
        "        references = [reference_text] * len(generated_texts)\n",
        "        print(\"No reference texts provided. Using first generated text as reference.\")\n",
        "\n",
        "    try:\n",
        "        print(\"Calculating BERTScore on sample (this is optimized but may still take a minute)...\")\n",
        "        # Use optimized BERTScore settings for speed\n",
        "        scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True, use_fast_tokenizer=True)\n",
        "        P, R, F1 = scorer.score(generated_texts, references)\n",
        "\n",
        "        bert_scores = {\n",
        "            \"bert_precision\": P.mean().item(),\n",
        "            \"bert_recall\": R.mean().item(),\n",
        "            \"bert_f1\": F1.mean().item()\n",
        "        }\n",
        "\n",
        "        # Save summary to JSON\n",
        "        with open(os.path.join(eval_output_dir, \"bertscore_summary.json\"), 'w') as f:\n",
        "            json.dump(bert_scores, f, indent=2)\n",
        "\n",
        "        return bert_scores\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating BERTScore: {e}\")\n",
        "        return {}\n",
        "\n",
        "# Main evaluation function\n",
        "def run_evaluation(model_name_or_path, test_prompts, reference_texts=None, sample_size=None):\n",
        "    \"\"\"Run full evaluation pipeline with optimized settings\"\"\"\n",
        "    print(f\"Starting evaluation with model: {model_name_or_path}\")\n",
        "\n",
        "    # Sample prompts if requested\n",
        "    if sample_size and sample_size < len(test_prompts):\n",
        "        print(f\"Sampling {sample_size} prompts from {len(test_prompts)} available prompts\")\n",
        "        eval_prompts = random.sample(test_prompts, sample_size)\n",
        "    else:\n",
        "        eval_prompts = test_prompts\n",
        "\n",
        "    # Load model\n",
        "    model, tokenizer = load_model(model_name_or_path)\n",
        "\n",
        "    # Generate SOPs in batches\n",
        "    generated_sops = batch_generate_sops(eval_prompts, model, tokenizer)\n",
        "\n",
        "    # Save generated SOPs\n",
        "    sops_path = os.path.join(eval_output_dir, \"generated_sops.json\")\n",
        "    with open(sops_path, 'w') as f:\n",
        "        json.dump(generated_sops, f, indent=2)\n",
        "\n",
        "    # Initialize evaluator and run evaluation\n",
        "    evaluator = FastSOPEvaluator(reference_texts)\n",
        "    evaluator.batch_evaluate(generated_sops)\n",
        "    summary = evaluator.generate_report()\n",
        "\n",
        "    # Run BERTScore on a small sample\n",
        "    bert_sample_size = min(10, len(generated_sops))\n",
        "    bert_scores = fast_bert_evaluation(generated_sops, reference_texts, bert_sample_size)\n",
        "\n",
        "    if bert_scores:\n",
        "        # Update summary with BERTScore\n",
        "        summary.update(bert_scores)\n",
        "        with open(os.path.join(eval_output_dir, \"evaluation_summary.json\"), 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "    # Generate final report\n",
        "    create_final_report(summary, generated_sops)\n",
        "\n",
        "    print(f\"\\nEvaluation complete! Results saved to {eval_output_dir}\")\n",
        "    return summary, generated_sops\n",
        "\n",
        "def create_final_report(summary, generated_sops):\n",
        "    \"\"\"Create a concise final report\"\"\"\n",
        "    with open(os.path.join(eval_output_dir, \"evaluation_report.md\"), 'w') as f:\n",
        "        f.write(\"# SOP Generation Model Evaluation Report\\n\\n\")\n",
        "        f.write(f\"Evaluation performed on {len(generated_sops)} prompts\\n\\n\")\n",
        "\n",
        "        f.write(\"## Summary Metrics\\n\\n\")\n",
        "        f.write(\"| Metric | Value |\\n\")\n",
        "        f.write(\"|--------|-------|\\n\")\n",
        "        f.write(f\"| Average Length | {summary['avg_length']:.1f} words |\\n\")\n",
        "        f.write(f\"| Completeness Rate | {summary['completeness_rate']:.1f}% |\\n\")\n",
        "        f.write(f\"| Style Similarity | {summary['avg_style_similarity']:.3f} |\\n\")\n",
        "        f.write(f\"| Flesch Reading Ease | {summary['avg_flesch_reading_ease']:.1f} |\\n\")\n",
        "        f.write(f\"| Grade Level | {summary['avg_flesch_kincaid_grade']:.1f} |\\n\")\n",
        "\n",
        "        if \"bert_f1\" in summary:\n",
        "            f.write(f\"| BERTScore F1 | {summary['bert_f1']:.4f} |\\n\")\n",
        "\n",
        "        f.write(\"\\n## Example Generation\\n\\n\")\n",
        "        # Show first example\n",
        "        first_prompt = next(iter(generated_sops.keys()))\n",
        "        f.write(f\"**Prompt:** {first_prompt[:100]}...\\n\\n\")\n",
        "        f.write(\"**Generated SOP:**\\n\\n\")\n",
        "        f.write(f\"```\\n{generated_sops[first_prompt][:300]}...\\n```\\n\\n\")\n",
        "\n",
        "        f.write(\"\\n## Next Steps\\n\\n\")\n",
        "        f.write(\"- Review the `generated_sops.json` file for all model outputs\\n\")\n",
        "        f.write(\"- Check `evaluation_metrics.csv` for detailed per-prompt metrics\\n\")\n",
        "        f.write(\"- See visualizations in the output directory\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "OhzsuTNa_dVg",
        "outputId": "b71cb48d-5617-48fa-a28b-7b46fc0b512e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: syntax error near unexpected token `'punkt_tab','\n",
            "/bin/bash: -c: line 1: `nltk.download('punkt_tab', quiet=True)'\n",
            "Setting up environment...\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe following code shows how to use the evaluation pipeline.\\nReplace the values with your actual data.\\n\\nExample:\\n```python\\n# Your model path\\nmodel_name_or_path = \"llama3-70b-instruct\"  # or local path\\n\\n# Load your test prompts\\ntest_prompts = [\\n    \"Create an SOP for laboratory safety procedures\",\\n    \"Write an SOP for new employee onboarding\",\\n    # Add more prompts...\\n]\\n\\n# Optional: Load reference texts (if available)\\nreference_texts = [\\n    \"STATEMENT OF PURPOSE\\nThis document outlines...\",\\n    # Add more reference SOPs...\\n]\\n\\n# Run evaluation with sampling for speed\\nsample_size = 20  # Adjust based on available time\\nsummary, generated_sops = run_evaluation(\\n    model_name_or_path, \\n    test_prompts,\\n    reference_texts=reference_texts,\\n    sample_size=sample_size\\n)\\n```\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your model\n",
        "model_name_or_path = \"/content/drive/MyDrive/mistral_sop_finetuned\"\n",
        "\n",
        "# Define your test prompts\n",
        "test_prompts = [\n",
        "     \"Write me an SOP for pursuing a Master's Degree in Computer Science focusing on Artificial Intelligence.\",  # Similar to training\n",
        "    \"Write me an SOP for pursuing a Master's Degree in Data Science at MIT.\",  # New domain but similar degree\n",
        "    \"Write me an SOP for pursuing a PhD in Biology with focus on Genetics.\",  # Different degree and field\n",
        "    \"Write me an SOP for pursuing an MBA with concentration in Finance.\",  # Very different domain\n",
        "    \"Write me an SOP for a Master's in Fine Arts focusing on Digital Media.\",  # Creative field\n",
        "    \"Write me an SOP for crafting a graduate school application statement focusing on collaboration and leadership experiences.\",\n",
        "    \"Write me an SOP for pursuing a Master's in Computer Science focusing on Artificial Intelligence (AI) and Machine Learning (ML). Share your personal experiences and how they have influenced you towards choosing AI/ML as your area of interest.\"\n",
        "]\n",
        "\n",
        "# Optional: Add reference SOPs if available\n",
        "reference_texts = [\n",
        "    \"STATEMENT OF PURPOSE\\nThis document outlines...\",\n",
        "    # Add more reference SOPs...\n",
        "]\n",
        "\n",
        "# Run the evaluation (adjust sample size as needed)\n",
        "summary, generated_sops = run_evaluation(\n",
        "    model_name_or_path,\n",
        "    test_prompts,\n",
        "    reference_texts=reference_texts,\n",
        "    sample_size=20  # Evaluate 20 random prompts for speed\n",
        ")"
      ],
      "metadata": {
        "id": "yuZl1zGb_xxi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}